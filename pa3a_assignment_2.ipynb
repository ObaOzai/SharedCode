{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6706e584e49dbdcbb187fdb18abd3809",
          "grade": false,
          "grade_id": "cell-32d7abe8a42b6ac7",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "3G4n63Ki981I"
      },
      "source": [
        "# Assignment 3 Part A - Recurrent Neural Networks\n",
        "\n",
        "Welcome to the first part of the third assignment!\n",
        "\n",
        "We'll be covering the implementation of RNNs. The main ideas will be relatively understandable, but the implementations may get a little confusing. So read everything carefully!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0wL3jwXK981L"
      },
      "outputs": [],
      "source": [
        "# Make sure to run this cell without modifying anything in it!\n",
        "import numpy as np\n",
        "\n",
        "# Import the code in `mytorch/nn.py`\n",
        "#from mytorch import nn\n",
        "\n",
        "# Import function that checks correctness of your answers.\n",
        "#from tests import compare_to_answer\n",
        "\n",
        "# These iPython functions make it so imported code is automatically reimported here if they are changed\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.py\n",
        "import numpy as np\n",
        "\n",
        "class Tanh:\n",
        "    \"\"\"[Given] The Tanh activation function, similar to torch.nn.Tanh.\"\"\"\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the Tanh activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input array.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Output array after applying tanh.\n",
        "        \"\"\"\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backward(self, state):\n",
        "        \"\"\"\n",
        "        Backward pass for the Tanh activation function.\n",
        "        Calculates the derivative of tanh(x) with respect to x, which is 1 - tanh(x)^2.\n",
        "\n",
        "        Args:\n",
        "            state (np.array): The output of the forward pass (tanh(x)).\n",
        "\n",
        "        Returns:\n",
        "            np.array: The gradient with respect to the input of tanh.\n",
        "        \"\"\"\n",
        "        return 1 - (state**2)\n",
        "\n",
        "\n",
        "class RNNCell:\n",
        "    \"\"\"RNNCell, similar to torch.nn.RNNCell.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Integer representing # input features expected by layer\n",
        "        hidden_size (int): Integer representing # features in the outputted hidden state\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize activation function\n",
        "        self.activation = Tanh()\n",
        "\n",
        "        # Randomly initialize weights and biases (\"Kaiming Uniform\" init)\n",
        "        # bound = np.sqrt(1 / hidden_size)  # Common for uniform, but often 1/sqrt(fan_in) or 1/sqrt(fan_out)\n",
        "        # For RNNs, a common initialization is often smaller, for tanh, typically 1/sqrt(fan_in) where fan_in = input_size + hidden_size\n",
        "        # A more common practice for tanh with uniform is Xavier/Glorot initialization, which is sqrt(6 / (fan_in + fan_out))\n",
        "        # Given your 'bound' was based on hidden_size, let's refine this to be more standard.\n",
        "\n",
        "        # Xavier/Glorot uniform initialization (suitable for tanh)\n",
        "        # For weight_ih, fan_in = input_size, fan_out = hidden_size\n",
        "        bound_ih = np.sqrt(6 / (input_size + hidden_size))\n",
        "        self.weight_ih = np.random.uniform(low=-bound_ih, high=bound_ih, size=(hidden_size, input_size))\n",
        "        self.bias_ih = np.random.uniform(low=-bound_ih, high=bound_ih, size=(hidden_size,))\n",
        "\n",
        "        # For weight_hh, fan_in = hidden_size, fan_out = hidden_size\n",
        "        bound_hh = np.sqrt(6 / (hidden_size + hidden_size)) # or just bound_ih if you want to keep it simpler\n",
        "        self.weight_hh = np.random.uniform(low=-bound_hh, high=bound_hh, size=(hidden_size, hidden_size))\n",
        "        self.bias_hh = np.random.uniform(low=-bound_hh, high=bound_hh, size=(hidden_size,))\n",
        "\n",
        "\n",
        "        # Gradients, initialized to zeros. These will accumulate during backprop.\n",
        "        self.grad_weight_ih = np.zeros((hidden_size, input_size))\n",
        "        self.grad_weight_hh = np.zeros((hidden_size, hidden_size))\n",
        "\n",
        "        self.grad_bias_ih = np.zeros(hidden_size)\n",
        "        self.grad_bias_hh = np.zeros(hidden_size)\n",
        "\n",
        "        # Store intermediate values for backward pass\n",
        "        self.x_t = None\n",
        "        self.h_prev = None\n",
        "        self.y_t = None # Output of the linear combination before activation\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        \"\"\"RNNCell forward (single timestep)\n",
        "\n",
        "        Args:\n",
        "            x_t (np.array): (batch_size, input_size) current timestep's input\n",
        "            h_prev (np.array): (batch_size, hidden_size) the previous timestep's outputted hidden state\n",
        "\n",
        "        Returns:\n",
        "            np.array: (batch_size, hidden_size) current timestep's outputted hidden state (h_t)\n",
        "        \"\"\"\n",
        "        # Store for backward pass\n",
        "        self.x_t = x_t\n",
        "        self.h_prev = h_prev\n",
        "\n",
        "        # Calculate y_t: linear combination of inputs and previous hidden state\n",
        "        # (batch_size, input_size) @ (input_size, hidden_size) + (batch_size, hidden_size) @ (hidden_size, hidden_size)\n",
        "        # Transpose weights for correct dot product if input is (batch_size, features)\n",
        "        term_ih = np.dot(x_t, self.weight_ih.T) + self.bias_ih\n",
        "        term_hh = np.dot(h_prev, self.weight_hh.T) + self.bias_hh\n",
        "\n",
        "        # y_t is the sum before activation\n",
        "        self.y_t = term_ih + term_hh\n",
        "\n",
        "        # Calculate h_t: apply activation function to y_t\n",
        "        h_t = self.activation.forward(self.y_t)\n",
        "\n",
        "        return h_t\n",
        "\n",
        "    def backward(self, grad, h_t, x_t, h_prev):\n",
        "        \"\"\"RNNCell backward (single timestep)\n",
        "\n",
        "        Args:\n",
        "            grad (np.array): (batch_size, hidden_size) the gradient w.r.t. the output of the current hidden layer (dL/dh_t)\n",
        "            h_t (np.array): (batch_size, hidden_size) the output of the forward pass at this timestep (used for activation derivative)\n",
        "            x_t (np.array): (batch_size, input_size) current timestep's input (used for input weight gradient)\n",
        "            h_prev (np.array): (batch_size, hidden_size) the previous timestep's outputted hidden state (used for recurrent weight gradient)\n",
        "\n",
        "        Returns:\n",
        "            np.array, np.array: shaped (batch_size, input_size) and (batch_size, hidden_size)\n",
        "                                respectively, representing dL/dx_t and dL/dh_prev\n",
        "        \"\"\"\n",
        "        # Backprop through the activation function\n",
        "        # dL/dy_t = dL/dh_t * dh_t/dy_t = grad * activation.backward(h_t)\n",
        "        grad_through_activation = grad * self.activation.backward(h_t) # Element-wise multiplication\n",
        "\n",
        "        # Accumulate the gradients for the weights and biases\n",
        "        # Gradients are summed over the batch dimension\n",
        "\n",
        "        # dL/dW_ih = (dL/dy_t).T @ x_t\n",
        "        self.grad_weight_ih += np.dot(grad_through_activation.T, x_t)\n",
        "        # dL/db_ih = sum(dL/dy_t, axis=0)\n",
        "        self.grad_bias_ih += np.sum(grad_through_activation, axis=0)\n",
        "\n",
        "        # dL/dW_hh = (dL/dy_t).T @ h_prev\n",
        "        self.grad_weight_hh += np.dot(grad_through_activation.T, h_prev)\n",
        "        # dL/db_hh = sum(dL/dy_t, axis=0)\n",
        "        self.grad_bias_hh += np.sum(grad_through_activation, axis=0)\n",
        "\n",
        "        # Calculate gradients for the input and the previous hidden state\n",
        "        # dL/dx_t = (dL/dy_t) @ W_ih\n",
        "        dx = np.dot(grad_through_activation, self.weight_ih)\n",
        "        # dL/dh_prev = (dL/dy_t) @ W_hh\n",
        "        dh = np.dot(grad_through_activation, self.weight_hh)\n",
        "\n",
        "        return dx, dh\n",
        "\n",
        "\n",
        "class RNN:\n",
        "    \"\"\"RNN layer, similar to torch.nn.RNN.\n",
        "\n",
        "    Assume our version has `batch_first=True` and `bidirectional=False`.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Number of input features expected by layer\n",
        "        hidden_size (int): Number of features in the outputted hidden state\n",
        "        num_layers (int): Number of RNNCells to stack, s.t. each layer feeds its output into the next layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Initialize first RNNCell, then add on more if num_layers > 1\n",
        "        # The first layer takes input_size, subsequent layers take hidden_size as input\n",
        "        self.layers = [RNNCell(input_size, hidden_size)]\n",
        "        for l in range(num_layers - 1):\n",
        "            self.layers.append(RNNCell(hidden_size, hidden_size))\n",
        "\n",
        "    def forward(self, x, h_0=None):\n",
        "        \"\"\"Forward propagation over multiple RNNCells, multiple timesteps.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): (batch_size, seq_len, input_size)\n",
        "                          Input. A batch of sequences that each have `input_size` features at each timestep\n",
        "            h_0 (np.array): (num_layers, batch_size, hidden_size)\n",
        "                            Initial hidden state (useful if you have prior context to give to this layer).\n",
        "                            If not given, creates a zero array.\n",
        "\n",
        "        Returns:\n",
        "            output (np.array): (batch_size, seq_len, hidden_size)\n",
        "                               Output of the last cell for each timestep\n",
        "            h_n (np.array): (num_layers, batch_size, hidden_size)\n",
        "                            The hidden state at the last timestep for each layer in the batch\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # [Given] Initialize tensor to store every hidden state at each time step and cell of the rnn\n",
        "        # hiddens[t+1, l, b, h] stores h_t for timestep t, layer l, batch b.\n",
        "        # hiddens[0, :, :, :] stores h_0 for each layer.\n",
        "        hiddens = np.zeros((seq_len + 1, self.num_layers, batch_size, self.hidden_size))\n",
        "        if h_0 is not None: # If given, store the initial hidden state\n",
        "            hiddens[0,:,:,:] = h_0\n",
        "\n",
        "        # Store the inputs at each timestep for backprop (x_t for the first layer)\n",
        "        # We need to store previous layer's output (which is current layer's input) for backprop.\n",
        "        # This will be `x` for the first layer, and `hiddens[t+1, l-1, :, :]` for subsequent layers.\n",
        "        # Let's create a structure to store inputs to each RNNCell at each timestep.\n",
        "        # This will be `(seq_len, num_layers, batch_size, input_size_for_layer)`\n",
        "        # The input size for the first layer is input_size, for others it's hidden_size.\n",
        "        # To simplify storage, let's store (seq_len, num_layers, batch_size, max_dim)\n",
        "        # A simpler approach is to store the actual x_t and h_prev within each RNNCell during its forward pass,\n",
        "        # which you've already started doing by setting `self.x_t` and `self.h_prev` in `RNNCell.forward`.\n",
        "        # However, for RNN class backward, you'll need the sequence of these for each layer and timestep.\n",
        "        # So it's better to manage this explicitly here.\n",
        "\n",
        "        # We'll store inputs to each layer and timestep for backward pass\n",
        "        # This structure helps: inputs_to_cells[t][l] = input_array\n",
        "        # Initialize a list of lists to hold cell inputs at each (timestep, layer)\n",
        "        self.cell_inputs = [[None for _ in range(self.num_layers)] for _ in range(seq_len)]\n",
        "        self.cell_h_prevs = [[None for _ in range(self.num_layers)] for _ in range(seq_len)]\n",
        "\n",
        "        # Process input, timestep by timestep, layer by layer.\n",
        "        for t in range(seq_len):\n",
        "            # Input to the first layer at current timestep `t`\n",
        "            current_input_to_layer = x[:, t, :] # (batch_size, input_size)\n",
        "\n",
        "            for l in range(self.num_layers):\n",
        "                # Get the previous hidden state for the current cell\n",
        "                h_prev = hiddens[t, l, :, :] # (batch_size, hidden_size)\n",
        "\n",
        "                # Store current input and h_prev for backward pass\n",
        "                self.cell_inputs[t][l] = current_input_to_layer\n",
        "                self.cell_h_prevs[t][l] = h_prev\n",
        "\n",
        "                # Perform forward pass for the current RNNCell\n",
        "                h_t = self.layers[l].forward(current_input_to_layer, h_prev)\n",
        "\n",
        "                # Store the new hidden state\n",
        "                hiddens[t+1, l, :, :] = h_t\n",
        "\n",
        "                # The output of the current layer becomes the input for the next layer\n",
        "                current_input_to_layer = h_t # (batch_size, hidden_size)\n",
        "\n",
        "        # [Given] Save the original input and hidden vectors we used, for backprop later.\n",
        "        self.x = x\n",
        "        self.hiddens = hiddens\n",
        "\n",
        "        # [Given] Return the output and final hidden states (we transpose to make the batch_size dim first)\n",
        "        # output is the last layer's hidden states for all timesteps (excluding h_0)\n",
        "        output = hiddens[1:, -1, :, :].transpose(1, 0, 2)\n",
        "        # h_n is the hidden state at the last timestep for each layer\n",
        "        h_n = hiddens[-1, :, :, :]\n",
        "\n",
        "        return output, h_n\n",
        "\n",
        "    def backward(self, grad):\n",
        "        \"\"\"Back Propagation Through Time (BPTT) through multiple RNNCells.\n",
        "\n",
        "        Args:\n",
        "            grad (np.array): (batch_size, hidden_size)\n",
        "                             Gradient of the loss w.r.t. output of last RNN cell at the last timestep (dL/dh_{T, L})\n",
        "\n",
        "        Returns:\n",
        "            dx (np.array): (batch_size, seq_len, input_size)\n",
        "                           Gradient of loss w.r.t. input\n",
        "            dh_0 (np.array) : (num_layers, batch_size, hidden_size)\n",
        "                              Gradient of loss w.r.t. the initial hidden state h_0\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_size = self.x.shape\n",
        "\n",
        "        # [Given] Initialize gradients of the input and initial hidden state\n",
        "        dx = np.zeros((batch_size, seq_len, input_size))\n",
        "        dh_0 = np.zeros((self.num_layers, batch_size, self.hidden_size))\n",
        "\n",
        "        # Initialize dh_current_layer_timestep: This will accumulate gradients from next layer and next timestep\n",
        "        # This effectively holds dL/dh_{t,l} as we backpropagate.\n",
        "        # It needs to be initialized with the gradient from the output layer (grad) for the last timestep/layer.\n",
        "\n",
        "        # Initialize the gradient flowing backwards from the final output `grad`\n",
        "        # This `grad_h` represents dL/dh_t for the current (t, l) being processed.\n",
        "        # Initially, it's the gradient from the loss w.r.t. the final hidden state `h_n`.\n",
        "        grad_h_current = np.zeros((self.num_layers, batch_size, self.hidden_size))\n",
        "        grad_h_current[-1, :, :] = grad # This is dL/dh_{T, L} where T=last timestep, L=last layer\n",
        "\n",
        "        # Iterate backwards through timesteps\n",
        "        for t in reversed(range(seq_len)):\n",
        "            # Initialize d_input_to_next_layer for this timestep.\n",
        "            # This accumulates gradients that need to be passed down to the previous layer.\n",
        "            # It starts with the gradient coming from the next layer (or 0 if this is the last layer being processed from top).\n",
        "            # It will be updated by the RNNCell.backward's 'dx' output.\n",
        "            grad_from_next_layer = np.zeros((batch_size, self.hidden_size)) # This is dL/dh_{t, l+1} essentially\n",
        "\n",
        "            # Iterate backwards through layers\n",
        "            for l in reversed(range(self.num_layers)):\n",
        "                # Get the relevant hidden states and inputs for this cell's backward pass\n",
        "                h_t = self.hiddens[t+1, l, :, :]       # Current cell's output (h_t)\n",
        "                h_prev_t = self.hiddens[t, l, :, :]     # Current cell's previous hidden state (h_{t-1})\n",
        "                x_t_for_cell = self.cell_inputs[t][l]   # Input to this cell at this timestep\n",
        "\n",
        "                # The 'grad' passed to RNNCell.backward is the sum of gradients from:\n",
        "                # 1. The output of the current cell (dL/dh_t from the downstream loss)\n",
        "                # 2. The gradient from the next layer (dL/dh_{t, l+1})\n",
        "                # 3. The gradient from the next timestep (dL/dh_{t+1, l}) - handled by grad_h_current\n",
        "\n",
        "                # The total gradient for h_t is `grad_h_current[l, :, :]` (gradient flowing from next timestep)\n",
        "                # PLUS `grad_from_next_layer` (gradient flowing from next layer).\n",
        "                # Note: `grad_from_next_layer` accumulates the `dx` from the layer above.\n",
        "                # If it's the last layer (`l == self.num_layers - 1`), then `grad_from_next_layer`\n",
        "                # comes from the 'grad' provided to RNN.backward.\n",
        "                # However, the structure of `grad` in RNN.backward is only `dL/dh_{T,L}`.\n",
        "                # Let's adjust `grad_h_current` to hold the total gradient from `L` and `T` for each (l,t).\n",
        "\n",
        "                # The gradient for h_t at this specific (t, l) comes from:\n",
        "                # 1. The initial 'grad' passed to RNN.backward (only if t is last_t and l is last_l).\n",
        "                # 2. The gradient from the *next layer* at the *current timestep* (passed as dx from layer l+1).\n",
        "                # 3. The gradient from the *next timestep* in the *current layer* (passed as dh from cell at t+1).\n",
        "\n",
        "                current_grad_for_cell = grad_h_current[l, :, :] + grad_from_next_layer\n",
        "\n",
        "                # Perform backward pass for the current RNNCell\n",
        "                # It returns dL/dx_t_cell (gradient w.r.t. input to this cell) and dL/dh_prev_cell (gradient w.r.t. h_prev)\n",
        "                dx_cell, dh_prev_cell = self.layers[l].backward(\n",
        "                    current_grad_for_cell, h_t, x_t_for_cell, h_prev_t\n",
        "                )\n",
        "\n",
        "                # Store the gradients:\n",
        "                # dL/dh_prev_cell becomes the gradient for h_{t-1} in this layer, which is added to grad_h_current\n",
        "                grad_h_current[l, :, :] = dh_prev_cell\n",
        "\n",
        "                # dL/dx_cell becomes the gradient for the input to this cell.\n",
        "                # If it's the first layer (l=0), this is dL/dx[:, t, :].\n",
        "                # If it's not the first layer, this is dL/dh_{t, l-1}, which will be added to the grad_from_next_layer for the layer below.\n",
        "                if l == 0:\n",
        "                    dx[:, t, :] = dx_cell # Gradient for the original input `x`\n",
        "                else:\n",
        "                    grad_from_next_layer = dx_cell # This will be added to `current_grad_for_cell` for layer `l-1`\n",
        "\n",
        "        # The final dh_0 is what's left in grad_h_current for the first timestep (t=0)\n",
        "        # dh_0 should be `hiddens[0,:,:,:]` after backprop.\n",
        "        # It's already accumulated in `grad_h_current` through the loop\n",
        "        dh_0 = grad_h_current\n",
        "\n",
        "        return dx, dh_0"
      ],
      "metadata": {
        "id": "cDrmAmJS_HO4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tests.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ------------\n",
        "# Test methods\n",
        "# ------------\n",
        "\n",
        "def test_rnncell_forward_1(RNNCell):\n",
        "    layer = RNNCell(3, 4)\n",
        "    layer.weight_ih = np.array([[-0.01, -0.02, -0.03],\n",
        "                                [0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06],\n",
        "                                [0.07, 0.08, 0.09]])\n",
        "    layer.bias_ih = np.array([[-0.01, -0.02, 0.03, 0.04]])\n",
        "    layer.weight_hh = np.array([[-0.08, -0.07, -0.06, -0.05],\n",
        "                                [-0.04, -0.03, -0.02, -0.01],\n",
        "                                [0., 0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06, 0.07]])\n",
        "    layer.bias_hh = np.array([[0.01, 0.02, -0.03, -0.04]])\n",
        "    x = np.array([[1., 2., 3.],\n",
        "                  [4., 5., 6.]])\n",
        "    h_prev = np.array([[1., 2., 3., 4.],\n",
        "                    [3., 2., 1., 0.]])\n",
        "\n",
        "    out = layer.forward(x, h_prev)\n",
        "    return out\n",
        "\n",
        "def test_rnncell_forward_2(RNNCell):\n",
        "    layer = RNNCell(2, 5)\n",
        "    layer.weight_ih = np.array([[-0.005, -5.,],\n",
        "                                [0., 1.,],\n",
        "                                [3., 4.,],\n",
        "                                [6., 2.,],\n",
        "                                [0., 0.]])\n",
        "    layer.bias_ih = np.array([[-0.0002, -2., 1., 1., 0.]])\n",
        "    layer.weight_hh = np.array([[-5., -4., -3., -2., -1.],\n",
        "                                [-4., -3., -2., -1., 0.],\n",
        "                                [0., 1., 2., 3., 0.],\n",
        "                                [4., 5., 6., 7., 6.],\n",
        "                                [7., 8., 9., -0.25, 0.25]])\n",
        "    layer.bias_hh = np.array([[0.001, 2., -3., -4., 0.]])\n",
        "    x = np.array([[0.0005, 2.],\n",
        "                  [-0.152, -0.025],\n",
        "                  [0., -1.]])\n",
        "    h_prev = np.array([[0.02, 0.0005, 0.15, 0.45, 0.25],\n",
        "                       [-0.001, 2., 1., 0., -0.25],\n",
        "                       [1., -2., -5., -10., 0.]])\n",
        "\n",
        "    out = layer.forward(x, h_prev)\n",
        "    return out\n",
        "\n",
        "def test_rnncell_forward_3(RNNCell):\n",
        "    layer = RNNCell(5, 2)\n",
        "    layer.weight_ih = np.array([[-0.005, -0.01, 0.05, 0.15, 0.222],\n",
        "                                [0.151, 1.10, 0.015, -0.002, 0.051]])\n",
        "    layer.bias_ih = np.array([[0.11, -0.15]])\n",
        "    layer.weight_hh = np.array([[-0.152, 0.112],\n",
        "                                [-0.002, -1.01]])\n",
        "    layer.bias_hh = np.array([[0.001, 2.]])\n",
        "    x = np.array([[-0.001, -2.015, -0.125, -0.001, 1.025],\n",
        "                  [0.122, -0.128, -0.0002, -0.01, 0.15],\n",
        "                  [-0.01, -0.002, 0.0152, 0.123, 0.231],\n",
        "                  [-0.821, 0.999, 0.251, 0.331, 0.025]])\n",
        "    h_prev = np.array([[0.02, 0.0005],\n",
        "                       [-0.001, 2.],\n",
        "                       [1., -2.],\n",
        "                       [0.025, 0.152]])\n",
        "\n",
        "    out = layer.forward(x, h_prev)\n",
        "    return out\n",
        "\n",
        "def test_rnncell_backward_1(RNNCell):\n",
        "    layer = RNNCell(3, 4)\n",
        "    layer.weight_ih = np.array([[-0.01, -0.02, -0.03],\n",
        "                                [0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06],\n",
        "                                [0.07, 0.08, 0.09]])\n",
        "    layer.bias_ih = np.array([[-0.01, -0.02, 0.03, 0.04]])\n",
        "    layer.weight_hh = np.array([[-0.08, -0.07, -0.06, -0.05],\n",
        "                                [-0.04, -0.03, -0.02, -0.01],\n",
        "                                [0., 0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06, 0.07]])\n",
        "    layer.bias_hh = np.array([[0.01, 0.02, -0.03, -0.04]])\n",
        "    h_prev = np.array([[1., 2., 3., 4.],\n",
        "                    [3., 2., 1., 0.]])\n",
        "\n",
        "    grad = np.array([[ 0.402888  ,  0.01693988,  0.04669028,  0.54219921],\n",
        "                     [-0.88253697,  0.11543817, -0.70723666, -0.69393529]])\n",
        "    h_prev_l = np.array([[-0.62162275, -0.96164911,  0.21242244],\n",
        "                         [ 0.27069328, -0.83331687, -0.6866582 ]])\n",
        "    h_prev_t = np.array([[ 0.33983293,  0.93178091,  0.69990522, -0.15981829],\n",
        "                         [ 0.12239934, -0.56387259, -0.9556718 , -0.31575391]])\n",
        "\n",
        "    dx, dh = layer.backward(grad, h_prev, h_prev_l, h_prev_t)\n",
        "    return dx, dh, layer.grad_weight_ih, layer.grad_weight_hh, layer.grad_bias_ih, layer.grad_bias_hh\n",
        "\n",
        "\n",
        "def test_rnncell_backward_2(RNNCell):\n",
        "    layer = RNNCell(2, 5)\n",
        "    layer.weight_ih = np.array([[-0.005, -5.,],\n",
        "                                [0., 1.,],\n",
        "                                [3., 4.,],\n",
        "                                [6., 2.,],\n",
        "                                [0., 0.]])\n",
        "    layer.bias_ih = np.array([[-0.0002, -2., 1., 1., 0.]])\n",
        "    layer.weight_hh = np.array([[-5., -4., -3., -2., -1.],\n",
        "                                [-4., -3., -2., -1., 0.],\n",
        "                                [0., 1., 2., 3., 0.],\n",
        "                                [4., 5., 6., 7., 6.],\n",
        "                                [7., 8., 9., -0.25, 0.25]])\n",
        "    layer.bias_hh = np.array([[0.001, 2., -3., -4., 0.]])\n",
        "\n",
        "    h_prev = np.array([[0.02, 0.0005, 0.15, 0.45, 0.25],\n",
        "                       [-0.001, 2., 1., 0., -0.25],\n",
        "                       [1., -2., -5., -10., 0.]])\n",
        "\n",
        "    grad = np.array([[-0.9207447 , -0.0299716 ,  0.93634652, -0.75164411, -0.73439064],\n",
        "       [-0.09640495, -0.87471139,  0.72295136,  0.8407652 ,  0.94054401],\n",
        "       [ 0.08871341, -0.30203741,  0.57196294, -0.05680726, -0.59568463]])\n",
        "    h_prev_l = np.array([[-0.66812673,  0.54131346],\n",
        "       [-0.09802792,  0.56934838],\n",
        "       [-0.0081655 ,  0.59356404]])\n",
        "    h_prev_t = np.array([[ 0.72465154,  0.04605758,  0.44489962, -0.81581832, -0.46412398],\n",
        "       [-0.11491536, -0.90658557, -0.06506369,  0.93885842, -0.88547036],\n",
        "       [-0.86381171, -0.59843711, -0.63817132, -0.06270253,  0.22630636]])\n",
        "\n",
        "    dx, dh = layer.backward(grad, h_prev, h_prev_l, h_prev_t)\n",
        "    return dx, dh, layer.grad_weight_ih, layer.grad_weight_hh, layer.grad_bias_ih, layer.grad_bias_hh\n",
        "\n",
        "def test_rnncell_backward_3(RNNCell):\n",
        "    layer = RNNCell(5, 2)\n",
        "    layer.weight_ih = np.array([[-0.005, -0.01, 0.05, 0.15, 0.222],\n",
        "                                [0.151, 1.10, 0.015, -0.002, 0.051]])\n",
        "    layer.bias_ih = np.array([[0.11, -0.15]])\n",
        "    layer.weight_hh = np.array([[-0.152, 0.112],\n",
        "                                [-0.002, -1.01]])\n",
        "    layer.bias_hh = np.array([[0.001, 2.]])\n",
        "    h_prev = np.array([[0.02, 0.0005],\n",
        "                       [-0.001, 2.],\n",
        "                       [1., -2.],\n",
        "                       [0.025, 0.152]])\n",
        "\n",
        "    grad = np.array([[-0.19521881, -0.82817359],\n",
        "       [-0.52215925, -0.16664658],\n",
        "       [ 0.82267516, -0.52842318],\n",
        "       [-0.78720939, -0.11186485]])\n",
        "    h_prev_l = np.array([[-0.10070971, -0.14045025, -0.31836873,  0.22785118, -0.40093625],\n",
        "       [ 0.01313353, -0.05711794,  0.37709152, -0.80443835,  0.53103029],\n",
        "       [ 0.84935059,  0.00150989, -0.76366634,  0.10226739, -0.83378106],\n",
        "       [-0.71682395,  0.10407237,  0.84127296,  0.25157057, -0.28898927]])\n",
        "    h_prev_t = np.array([[-0.28169733,  0.05609386],\n",
        "       [ 0.66147383, -0.41036188],\n",
        "       [ 0.51908581, -0.31614092],\n",
        "       [-0.337796  , -0.46155793]])\n",
        "\n",
        "    dx, dh = layer.backward(grad, h_prev, h_prev_l, h_prev_t)\n",
        "    return dx, dh, layer.grad_weight_ih, layer.grad_weight_hh, layer.grad_bias_ih, layer.grad_bias_hh\n",
        "\n",
        "def test_rnn_forward_1(RNN):\n",
        "    rnn = RNN(3, 4, num_layers=2)\n",
        "\n",
        "    rnn.layers[0].weight_ih = np.array([[-0.01, -0.02, -0.03],\n",
        "                                [0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06],\n",
        "                                [0.07, 0.08, 0.09]])\n",
        "    rnn.layers[0].bias_ih = np.array([[-0.01, -0.02, 0.03, 0.04]])\n",
        "\n",
        "    rnn.layers[0].weight_hh = np.array([[-0.08, -0.07, -0.06, -0.05],\n",
        "                                [-0.04, -0.03, -0.02, -0.01],\n",
        "                                [0., 0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06, 0.07]])\n",
        "    rnn.layers[0].bias_hh = np.array([[0.01, 0.02, -0.03, -0.04]])\n",
        "\n",
        "    rnn.layers[1].weight_ih = np.array([[-0.48600084,  0.56108125,  0.60350991, -0.12398511],\n",
        "       [-0.61426125,  0.74533839,  0.28829775,  0.39298129],\n",
        "       [-0.23660597,  0.60180463, -0.17925026, -0.91010067],\n",
        "       [ 0.32842213,  0.88810569,  0.47370219, -0.02676785]])\n",
        "    rnn.layers[1].bias_ih = np.array([[ 0.80336632,  0.5080941 , -0.02041526, -0.23457551]])\n",
        "    rnn.layers[1].weight_hh = np.array([[-0.66123341,  0.89060331,  0.52273452,  0.51176128],\n",
        "       [ 0.56754451, -0.00703578,  0.7770004 ,  0.27559471],\n",
        "       [ 0.55518779, -0.73064323,  0.9099351 ,  0.57656225],\n",
        "       [ 0.36056783, -0.05294811,  0.47201524, -0.4093564 ]])\n",
        "    rnn.layers[1].bias_hh = np.array([[-0.03656752, -0.7702112 ,  0.43095679,  0.89191036]])\n",
        "\n",
        "    x = np.array([[[ 0.54963061, -0.05740221, -0.18798255],\n",
        "        [-0.05139327,  0.31849613, -0.71975814],\n",
        "        [ 0.25741578,  0.5026123 ,  0.64851701],\n",
        "        [ 0.15833562,  0.45382923,  0.17070994],\n",
        "        [-0.02513259,  0.7292614 ,  0.84537154]],\n",
        "\n",
        "       [[ 0.96812713, -0.77282445, -0.95171846],\n",
        "        [ 0.38731339, -0.83533687,  0.99131245],\n",
        "        [ 0.04599233, -0.95944108,  0.23168402],\n",
        "        [ 0.16541892,  0.13714912, -0.95989072],\n",
        "        [ 0.51472085,  0.37945332,  0.4281448 ]]])\n",
        "    h_0 = np.array([[[-0.71008325, -0.07277072,  0.78528585, -0.35171659],\n",
        "        [ 0.21250086,  0.90839637, -0.7255993 ,  0.53047081]],\n",
        "\n",
        "       [[-0.86511414, -0.94116341, -0.36088932,  0.69915346],\n",
        "        [-0.53193134,  0.29990252, -0.27214273, -0.72523786]]])\n",
        "\n",
        "    out, hiddens = rnn.forward(x, h_0)\n",
        "    return out, hiddens\n",
        "\n",
        "def test_rnn_forward_2(RNN):\n",
        "    rnn = RNN(2, 5, num_layers=1)\n",
        "\n",
        "    rnn.layers[0].weight_ih = np.array([[-0.005, -5.,],\n",
        "                                [0., 1.,],\n",
        "                                [3., 4.,],\n",
        "                                [6., 2.,],\n",
        "                                [0., 0.]])\n",
        "    rnn.layers[0].bias_ih = np.array([[-0.0002, -2., 1., 1., 0.]])\n",
        "    rnn.layers[0].weight_hh = np.array([[-5., -4., -3., -2., -1.],\n",
        "                                [-4., -3., -2., -1., 0.],\n",
        "                                [0., 1., 2., 3., 0.],\n",
        "                                [4., 5., 6., 7., 6.],\n",
        "                                [7., 8., 9., -0.25, 0.25]])\n",
        "    rnn.layers[0].bias_hh = np.array([[0.001, 2., -3., -4., 0.]])\n",
        "    x = np.array([[[-0.37172187,  0.75289209],\n",
        "            [-0.74297175, -0.02525347],\n",
        "            [ 0.53391313, -0.67597502],\n",
        "            [-0.7237738 , -0.26039564],\n",
        "            [-0.80266149,  0.16000827]],\n",
        "\n",
        "        [[ 0.21733297,  0.63915498],\n",
        "            [-0.29721541, -0.99389827],\n",
        "            [-0.41202637, -0.26365921],\n",
        "            [-0.12466216, -0.60468756],\n",
        "            [ 0.93159041, -0.78198452]],\n",
        "\n",
        "        [[-0.87435205, -0.84892116],\n",
        "            [ 0.46827638,  0.34440284],\n",
        "            [ 0.88852153,  0.09396869],\n",
        "            [-0.9570812 , -0.00129956],\n",
        "            [ 0.24959137, -0.85093966]]])\n",
        "    h_0 = np.array([[0.02, 0.0005, 0.15, 0.45, 0.25],\n",
        "                       [-0.001, 2., 1., 0., -0.25],\n",
        "                       [1., -2., -5., -10., 0.]])\n",
        "\n",
        "    out, hiddens = rnn.forward(x, h_0)\n",
        "    return out, hiddens\n",
        "\n",
        "def test_rnn_forward_3(RNN):\n",
        "    rnn = RNN(5, 2, num_layers=3)\n",
        "    rnn.layers[0].weight_ih = np.array([[-0.005, -0.01, 0.05, 0.15, 0.222],\n",
        "                                        [0.151, 1.10, 0.015, -0.002, 0.051]])\n",
        "    rnn.layers[0].bias_ih = np.array([[0.11, -0.15]])\n",
        "    rnn.layers[0].weight_hh = np.array([[-0.152, 0.112],\n",
        "                                        [-0.002, -1.01]])\n",
        "    rnn.layers[0].bias_hh = np.array([[0.001, 2.]])\n",
        "\n",
        "\n",
        "    rnn.layers[1].weight_ih = np.array([[ 0.00372146, -0.82457555],\n",
        "       [ 0.87680038, -0.46820705]])\n",
        "    rnn.layers[1].bias_ih = np.array([ 0.6543994 , -0.49759433])\n",
        "    rnn.layers[1].weight_hh = np.array([[-0.49185334, -0.22758022],\n",
        "       [ 0.14179349,  0.74545256]])\n",
        "    rnn.layers[1].bias_hh = np.array([ 0.10898453, -0.62921271])\n",
        "\n",
        "    rnn.layers[2].weight_ih = np.array([[ 0.43795292, -0.39165178],\n",
        "       [-0.43341205,  0.97362009]])\n",
        "    rnn.layers[2].bias_ih = np.array([-0.30573836,  0.65735053])\n",
        "    rnn.layers[2].weight_hh = np.array([[-0.03211809,  0.92450192],\n",
        "       [-0.8617065 ,  0.03840494]])\n",
        "    rnn.layers[2].bias_hh = np.array([-0.77441866,  0.05598034])\n",
        "\n",
        "\n",
        "    x = np.array([[[-0.17857984, -0.93947819, -0.67213643, -0.69538802,\n",
        "          0.20826295],\n",
        "        [ 0.70933672,  0.55675034, -0.31722017,  0.96747612,\n",
        "          0.69028081],\n",
        "        [ 0.92119125, -0.13467066, -0.35187102, -0.20076277,\n",
        "         -0.72281168]],\n",
        "\n",
        "       [[ 0.36549078, -0.40412135, -0.50507736, -0.96217543,\n",
        "         -0.68987774],\n",
        "        [ 0.47556309, -0.21759229,  0.88103497, -0.77756477,\n",
        "         -0.47552185],\n",
        "        [-0.21578696,  0.55293293,  0.68532053,  0.78959926,\n",
        "         -0.56555195]]])\n",
        "    h_0 = np.array([[[-0.81155394, -0.67203904],\n",
        "        [ 0.21009179, -0.28347711]],\n",
        "\n",
        "       [[ 0.43134709,  0.19829147],\n",
        "        [-0.20346234,  0.59861199]],\n",
        "\n",
        "       [[-0.73887404, -0.843644  ],\n",
        "        [-0.36426267,  0.3433386 ]]])\n",
        "\n",
        "    out, hiddens = rnn.forward(x, h_0)\n",
        "    return out, hiddens\n",
        "\n",
        "def test_rnn_backward_1(RNN):\n",
        "    rnn = RNN(3, 4, num_layers=2)\n",
        "\n",
        "    rnn.layers[0].weight_ih = np.array([[-0.01, -0.02, -0.03],\n",
        "                                [0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06],\n",
        "                                [0.07, 0.08, 0.09]])\n",
        "    rnn.layers[0].bias_ih = np.array([[-0.01, -0.02, 0.03, 0.04]])\n",
        "\n",
        "    rnn.layers[0].weight_hh = np.array([[-0.08, -0.07, -0.06, -0.05],\n",
        "                                [-0.04, -0.03, -0.02, -0.01],\n",
        "                                [0., 0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06, 0.07]])\n",
        "    rnn.layers[0].bias_hh = np.array([[0.01, 0.02, -0.03, -0.04]])\n",
        "\n",
        "    rnn.layers[1].weight_ih = np.array([[-0.48600084,  0.56108125,  0.60350991, -0.12398511],\n",
        "       [-0.61426125,  0.74533839,  0.28829775,  0.39298129],\n",
        "       [-0.23660597,  0.60180463, -0.17925026, -0.91010067],\n",
        "       [ 0.32842213,  0.88810569,  0.47370219, -0.02676785]])\n",
        "    rnn.layers[1].bias_ih = np.array([[ 0.80336632,  0.5080941 , -0.02041526, -0.23457551]])\n",
        "    rnn.layers[1].weight_hh = np.array([[-0.66123341,  0.89060331,  0.52273452,  0.51176128],\n",
        "       [ 0.56754451, -0.00703578,  0.7770004 ,  0.27559471],\n",
        "       [ 0.55518779, -0.73064323,  0.9099351 ,  0.57656225],\n",
        "       [ 0.36056783, -0.05294811,  0.47201524, -0.4093564 ]])\n",
        "    rnn.layers[1].bias_hh = np.array([[-0.03656752, -0.7702112 ,  0.43095679,  0.89191036]])\n",
        "\n",
        "    x = np.array([[[ 0.54963061, -0.05740221, -0.18798255],\n",
        "        [-0.05139327,  0.31849613, -0.71975814],\n",
        "        [ 0.25741578,  0.5026123 ,  0.64851701],\n",
        "        [ 0.15833562,  0.45382923,  0.17070994],\n",
        "        [-0.02513259,  0.7292614 ,  0.84537154]],\n",
        "\n",
        "       [[ 0.96812713, -0.77282445, -0.95171846],\n",
        "        [ 0.38731339, -0.83533687,  0.99131245],\n",
        "        [ 0.04599233, -0.95944108,  0.23168402],\n",
        "        [ 0.16541892,  0.13714912, -0.95989072],\n",
        "        [ 0.51472085,  0.37945332,  0.4281448 ]]])\n",
        "    h_0 = np.array([[[-0.71008325, -0.07277072,  0.78528585, -0.35171659],\n",
        "        [ 0.21250086,  0.90839637, -0.7255993 ,  0.53047081]],\n",
        "\n",
        "       [[-0.86511414, -0.94116341, -0.36088932,  0.69915346],\n",
        "        [-0.53193134,  0.29990252, -0.27214273, -0.72523786]]])\n",
        "\n",
        "    rnn.forward(x, h_0)\n",
        "\n",
        "    grad = np.array([[-0.1, 0.2, -0.3, 0.4],\n",
        "                     [-0.4, 0.5, -0.6, 0.7]])\n",
        "\n",
        "    dx, dh_0 = rnn.backward(grad)\n",
        "\n",
        "    # Gather all the gradients in a single list\n",
        "    layer_gradients = []\n",
        "    for n in range(rnn.num_layers):\n",
        "        layer_gradients.append([rnn.layers[n].grad_weight_ih, rnn.layers[n].grad_weight_hh, rnn.layers[n].grad_bias_ih, rnn.layers[n].grad_bias_hh])\n",
        "\n",
        "    return dx, dh_0, layer_gradients\n",
        "\n",
        "def test_rnn_backward_2(RNN):\n",
        "    rnn = RNN(2, 5, num_layers=1)\n",
        "\n",
        "    rnn.layers[0].weight_ih = np.array([[-0.005, -5.,],\n",
        "                                [0., 1.,],\n",
        "                                [3., 4.,],\n",
        "                                [6., 2.,],\n",
        "                                [0., 0.]])\n",
        "    rnn.layers[0].bias_ih = np.array([[-0.0002, -2., 1., 1., 0.]])\n",
        "    rnn.layers[0].weight_hh = np.array([[-5., -4., -3., -2., -1.],\n",
        "                                [-4., -3., -2., -1., 0.],\n",
        "                                [0., 1., 2., 3., 0.],\n",
        "                                [4., 5., 6., 7., 6.],\n",
        "                                [7., 8., 9., -0.25, 0.25]])\n",
        "    rnn.layers[0].bias_hh = np.array([[0.001, 2., -3., -4., 0.]])\n",
        "    x = np.array([[[-0.37172187,  0.75289209],\n",
        "            [-0.74297175, -0.02525347],\n",
        "            [ 0.53391313, -0.67597502],\n",
        "            [-0.7237738 , -0.26039564],\n",
        "            [-0.80266149,  0.16000827]],\n",
        "\n",
        "        [[ 0.21733297,  0.63915498],\n",
        "            [-0.29721541, -0.99389827],\n",
        "            [-0.41202637, -0.26365921],\n",
        "            [-0.12466216, -0.60468756],\n",
        "            [ 0.93159041, -0.78198452]],\n",
        "\n",
        "        [[-0.87435205, -0.84892116],\n",
        "            [ 0.46827638,  0.34440284],\n",
        "            [ 0.88852153,  0.09396869],\n",
        "            [-0.9570812 , -0.00129956],\n",
        "            [ 0.24959137, -0.85093966]]])\n",
        "    h_0 = np.array([[0.02, 0.0005, 0.15, 0.45, 0.25],\n",
        "                       [-0.001, 2., 1., 0., -0.25],\n",
        "                       [1., -2., -5., -10., 0.]])\n",
        "\n",
        "    rnn.forward(x, h_0)\n",
        "\n",
        "    grad = np.array([[ 0.93211174, -0.38224902, -0.48032845,  0.74022526, -0.24474702],\n",
        "       [ 0.0575357 ,  0.86414712,  0.30018032,  0.95602853,  0.09027887],\n",
        "       [-0.62221448,  0.56082445, -0.75347751,  0.4888543 ,  0.80749852]])\n",
        "\n",
        "    dx, dh_0 = rnn.backward(grad)\n",
        "\n",
        "    # Gather all the gradients in a single list\n",
        "    layer_gradients = []\n",
        "    for n in range(rnn.num_layers):\n",
        "        layer_gradients.append([rnn.layers[n].grad_weight_ih, rnn.layers[n].grad_weight_hh, rnn.layers[n].grad_bias_ih, rnn.layers[n].grad_bias_hh])\n",
        "\n",
        "    return dx, dh_0, layer_gradients\n",
        "\n",
        "def test_rnn_backward_3(RNN):\n",
        "    rnn = RNN(5, 2, num_layers=3)\n",
        "    rnn.layers[0].weight_ih = np.array([[-0.005, -0.01, 0.05, 0.15, 0.222],\n",
        "                                        [0.151, 1.10, 0.015, -0.002, 0.051]])\n",
        "    rnn.layers[0].bias_ih = np.array([[0.11, -0.15]])\n",
        "    rnn.layers[0].weight_hh = np.array([[-0.152, 0.112],\n",
        "                                        [-0.002, -1.01]])\n",
        "    rnn.layers[0].bias_hh = np.array([[0.001, 2.]])\n",
        "\n",
        "\n",
        "    rnn.layers[1].weight_ih = np.array([[ 0.00372146, -0.82457555],\n",
        "       [ 0.87680038, -0.46820705]])\n",
        "    rnn.layers[1].bias_ih = np.array([ 0.6543994 , -0.49759433])\n",
        "    rnn.layers[1].weight_hh = np.array([[-0.49185334, -0.22758022],\n",
        "       [ 0.14179349,  0.74545256]])\n",
        "    rnn.layers[1].bias_hh = np.array([ 0.10898453, -0.62921271])\n",
        "\n",
        "    rnn.layers[2].weight_ih = np.array([[ 0.43795292, -0.39165178],\n",
        "       [-0.43341205,  0.97362009]])\n",
        "    rnn.layers[2].bias_ih = np.array([-0.30573836,  0.65735053])\n",
        "    rnn.layers[2].weight_hh = np.array([[-0.03211809,  0.92450192],\n",
        "       [-0.8617065 ,  0.03840494]])\n",
        "    rnn.layers[2].bias_hh = np.array([-0.77441866,  0.05598034])\n",
        "\n",
        "\n",
        "    x = np.array([[[-0.17857984, -0.93947819, -0.67213643, -0.69538802,\n",
        "          0.20826295],\n",
        "        [ 0.70933672,  0.55675034, -0.31722017,  0.96747612,\n",
        "          0.69028081],\n",
        "        [ 0.92119125, -0.13467066, -0.35187102, -0.20076277,\n",
        "         -0.72281168]],\n",
        "\n",
        "       [[ 0.36549078, -0.40412135, -0.50507736, -0.96217543,\n",
        "         -0.68987774],\n",
        "        [ 0.47556309, -0.21759229,  0.88103497, -0.77756477,\n",
        "         -0.47552185],\n",
        "        [-0.21578696,  0.55293293,  0.68532053,  0.78959926,\n",
        "         -0.56555195]]])\n",
        "    h_0 = np.array([[[-0.81155394, -0.67203904],\n",
        "        [ 0.21009179, -0.28347711]],\n",
        "\n",
        "       [[ 0.43134709,  0.19829147],\n",
        "        [-0.20346234,  0.59861199]],\n",
        "\n",
        "       [[-0.73887404, -0.843644  ],\n",
        "        [-0.36426267,  0.3433386 ]]])\n",
        "\n",
        "    rnn.forward(x, h_0)\n",
        "\n",
        "    grad = np.array([[ 0.65155603, -0.78930043],\n",
        "       [ 0.11431596,  0.36581844]])\n",
        "\n",
        "    dx, dh_0 = rnn.backward(grad)\n",
        "\n",
        "    # Gather all the gradients in a single list\n",
        "    layer_gradients = []\n",
        "    for n in range(rnn.num_layers):\n",
        "        layer_gradients.append([rnn.layers[n].grad_weight_ih, rnn.layers[n].grad_weight_hh, rnn.layers[n].grad_bias_ih, rnn.layers[n].grad_bias_hh])\n",
        "\n",
        "    return dx, dh_0, layer_gradients\n",
        "\n",
        "# ---------------------\n",
        "# General methods below\n",
        "# ---------------------\n",
        "\n",
        "def compare_to_answer(user_output, answer, test_name=None):\n",
        "    # Check that the object type of user's answer is correct\n",
        "    if not check_types_same(user_output, answer, test_name):\n",
        "        return False\n",
        "    # Check that the shape of the user's answer matches the expected shape\n",
        "    if not check_shapes_same(user_output, answer, test_name):\n",
        "        return False\n",
        "    # Check that the values of the user's answer matches the expected values\n",
        "    if not check_values_same(user_output, answer, test_name):\n",
        "        return False\n",
        "    # If passed all the above tests, return True\n",
        "    return True\n",
        "\n",
        "def check_types_same(user_output, answer, test_name=None):\n",
        "    try:\n",
        "        assert isinstance(user_output, type(answer))\n",
        "    except Exception as e:\n",
        "        if test_name:\n",
        "            print(f'Incorrect object type for {test_name}.')\n",
        "        print(\"Your output's type:\", type(user_output))\n",
        "        print(\"Expected type:\", type(answer))\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def check_shapes_same(user_output, answer, test_name=None):\n",
        "    try:\n",
        "        assert user_output.shape == answer.shape\n",
        "    except Exception as e:\n",
        "        if test_name:\n",
        "            print(f'Incorrect shape for {test_name}.')\n",
        "        print('Your shape:', user_output.shape)\n",
        "        print('Your values:\\n', user_output)\n",
        "        print('Expected shape:', answer.shape)\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def check_values_same(user_output, answer, test_name=None):\n",
        "    try:\n",
        "        assert np.allclose(user_output, answer)\n",
        "    except Exception as e:\n",
        "        if test_name:\n",
        "            print(f'Incorrect values for {test_name}.')\n",
        "        print('Your values:\\n', user_output)\n",
        "        return False\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "wELWa4xvAQtA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fbff1c7afb2b3ff7b8e49badd16f4971",
          "grade": false,
          "grade_id": "cell-148769ca3149b0c7",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "FPiJqf0I981M"
      },
      "source": [
        "# Section 1 - RNN Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7bd87e63019ba0f58e09f228137b7b52",
          "grade": false,
          "grade_id": "cell-997baf0de6e94f6d",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "DEMoYtQe981M"
      },
      "source": [
        "\n",
        "## Question 1.1 - `RNNCell.forward()`\n",
        "\n",
        "We'll begin by implementing a basic Elman RNN cell.\n",
        "\n",
        "In `mytorch/nn.py`, implement `RNNCell.forward()` using the following description:\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"images/rnn_cell.png\" width=\"250\"/>\n",
        "</div>\n",
        "\n",
        "Above is the a visualization of the cell. You can see the previous timestep's outputted hidden state $h_{t-1}$ on the top left, and the current timestep's input $x_t$ on the bottom. The green box is where our computations will happen.\n",
        "\n",
        "Calculate and `return` $h_t$ like so:\n",
        "\n",
        "$$\\begin{align*}\n",
        "    y_t &= x_t W^{T}_{ih}+b_{ih}+h_{(t-1)} W^{T}_{hh}+b_{hh} \\\\\n",
        "    h_t &= \\text{Tanh}(y_t)\n",
        "\\end{align*}$$\n",
        "\n",
        "$$\\begin{align*}\n",
        "    &\\text{Where $W^{T}_{ih}$ is a transposed weight matrix for processing the input data $x_t$ and} \\\\\n",
        "    &\\text{$W^{T}_{hh}$ is a transposed weight matrix for processing the previous hidden state $h_{(t-1)}$.} \\\\\n",
        "    &\\text{Tanh refers to the Tanh activation function.}\n",
        "\\end{align*}$$\n",
        "\n",
        "**Notes**:\n",
        "- We've provided you an implementation of `Tanh` in `nn.py`.\n",
        "- `Tanh` is also already initialized for you in `RNNCell.__init__()` as `self.activation`.\n",
        "    - When you want to use it, call `self.activation.forward()` with the appropriate input.\n",
        "- You can refer to the official [torch docs](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html) for this object if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9whdku18981M",
        "outputId": "7e092b3e-5192-4eb4-b2e3-836ce21cdaed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.        , -1.        ,  1.        ,  1.        ],\n",
              "       [-1.        , -0.99505475,  1.        ,  1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def test_rnncell_forward_1(RNNCell):\n",
        "    layer = RNNCell(3, 4)\n",
        "    layer.weight_ih = np.array([[-3., -2., -1.],\n",
        "                                [0., 1., 2.],\n",
        "                                [3., 4., 5.],\n",
        "                                [6., 7., 8.]])\n",
        "    layer.bias_ih = np.array([[-1., -2., 3., 4.]])\n",
        "    layer.weight_hh = np.array([[-8., -7., -6., -5.],\n",
        "                                [-4., -3., -2., -1.],\n",
        "                                [0., 1., 2., 3.],\n",
        "                                [4., 5., 6., 7.]])\n",
        "    layer.bias_hh = np.array([[1., 2., -3., -4.]])\n",
        "    x = np.array([[1., 2., 3.],\n",
        "                [4., 5., 6.]])\n",
        "    h_prev = np.array([[1., 2., 3., 4.],\n",
        "                    [3., 2., 1., 0.]])\n",
        "\n",
        "    out = layer.forward(x, h_prev)\n",
        "    return out\n",
        "\n",
        "test_rnncell_forward_1(RNNCell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVMIQeb8981N",
        "outputId": "4dac5b87-3bcb-4fb2-d461-02b1ff1acafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.         -1.          1.          1.        ]\n",
            " [-1.         -0.99505475  1.          1.        ]]\n",
            "[[-1.          0.82379071  0.99999955  0.99999655  0.89450007]\n",
            " [-1.         -0.99999978  0.89450007  1.          1.        ]\n",
            " [ 1.          1.         -1.         -1.         -1.        ]]\n",
            "[[ 0.33577323 -0.30660517]\n",
            " [ 0.35190119 -0.27725774]\n",
            " [-0.19202418  0.9991407 ]\n",
            " [ 0.18397031  0.99056243]]\n"
          ]
        }
      ],
      "source": [
        "#from tests import test_rnncell_forward_1, test_rnncell_forward_2, test_rnncell_forward_3\n",
        "\n",
        "answer_1 = test_rnncell_forward_1(RNNCell)\n",
        "answer_2 = test_rnncell_forward_2(RNNCell)\n",
        "answer_3 = test_rnncell_forward_3(RNNCell)\n",
        "print(answer_1)\n",
        "print(answer_2)\n",
        "print(answer_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "da11e5ff988e72ee35493c50c7f868cb",
          "grade": true,
          "grade_id": "cell-9da16a8204e24999",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "bETyXL8K981N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b8ae3f948c07d8bf40412341cb2c2382",
          "grade": false,
          "grade_id": "cell-124c35285bff3b5d",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "Nnu7qye9981N"
      },
      "source": [
        "If you passed the tests above, assign the string \"Question 1 passed\" to ans1 in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "48807a2ee6e21f71e5ab57909baf4d68",
          "grade": false,
          "grade_id": "cell-6c4275dd0f9fd0d1",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "xnklDVpg981N"
      },
      "outputs": [],
      "source": [
        "### GRADED\n",
        "\n",
        "ans1 = \"Question 1 passed\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "63822ec829d93a9854f3361080070f9b",
          "grade": true,
          "grade_id": "cell-4895e814caee312d",
          "locked": true,
          "points": 25,
          "schema_version": 3,
          "solution": false
        },
        "id": "D14DLllI981O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "175f584ae4f8a171e151ff1da4ed7708",
          "grade": false,
          "grade_id": "cell-0a675b78a0aeaf4d",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "686Zwzxs981O"
      },
      "source": [
        "## Question 1.2 - `RNNCell.backward()`\n",
        "\n",
        "Now for backprop. There are actually *seven* gradients we need to calculate.\n",
        "\n",
        "**Gradient of the loss w.r.t. input of activation**\n",
        "\n",
        "First, we need to calculate the gradient w.r.t. the input of the `Tanh` activation. We need this to calculate the other gradients.\n",
        "\n",
        "$$\\nabla_{y_t} \\text{Loss} = \\nabla_{y_t} h_{t} \\odot \\nabla_{h_{t}} \\text{Loss}$$\n",
        "\n",
        "$$\\begin{align*}\n",
        "    &\\text{Where $\\nabla_{y_t} h_{t}$ is the gradient of the layer's output $h_t$ w.r.t. the input of the activation function $y_t$,} \\\\\n",
        "    &\\text{and $\\odot$ is the element-wise multiplication operator.}\n",
        "\\end{align*}$$\n",
        "\n",
        "Notes:\n",
        "- You can use `self.activation.backward()`, but will have to give it a `state`.\n",
        "    - The `state` is the output of `RNNCell.forward()` at the current timestep.\n",
        "\n",
        "**Gradients of the loss w.r.t. the weights and biases at this timestep:**\n",
        "\n",
        "Now we need to calculate and store these four gradients:\n",
        "\n",
        "$$\\begin{align*}\n",
        "    \\nabla_{W_{ih}} \\text{Loss} += \\nabla_{y_t} \\text{Loss}^T h_{(t, l-1)} \\\\\n",
        "    \\nabla_{W_{hh}} \\text{Loss} += \\nabla_{y_t} \\text{Loss}^T h_{(t-1, l)} \\\\\n",
        "    \\nabla_{b_{ih}} \\text{Loss} += \\sum_{n=0}^{N-1}{\\nabla_{y_t} \\text{Loss}_n} \\\\\n",
        "    \\nabla_{b_{hh}} \\text{Loss} += \\sum_{n=0}^{N-1}{\\nabla_{y_t} \\text{Loss}_n}\n",
        "\\end{align*}$$\n",
        "\n",
        "Notes:\n",
        "- The first two formulas use matrix multiplications.\n",
        "- The last two formulas are simply summing across the `batch_size` axis, yielding a size `(hidden_size,)` array.\n",
        "- The last two formulas have identical right hand sides on purpose.\n",
        "    - Think about why this is; it has to do the partial derivatives w.r.t. each of those bias terms.\n",
        "- Notice the `+=` in the above equations.\n",
        "    - Similar to `nn.Conv1d` in assignment 2a, we use the weights and biases multiple times across different timesteps, so we need to accumulate their total influence over time.\n",
        "    - Backprop will pass through this layer for each time step it was used.\n",
        "        - It'll go in reverse order of time (last timestep -> first timestep)\n",
        "        - By the first timestep, it'll have finished accumulating the weight/bias gradients that represent the params' total influence on the output\n",
        "\n",
        "**Gradients of the loss w.r.t. input and prev hidden state:**\n",
        "\n",
        "Almost done! Calculate and `return` these two gradients:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\nabla_{x_t} \\text{Loss} = \\nabla_{y_t} \\text{Loss} W_{ih}\\\\\n",
        "\\nabla_{h_{(t-1)}} \\text{Loss} = \\nabla_{y_t} \\text{Loss} W_{hh}\n",
        "\\end{align*}$$\n",
        "\n",
        "Notes:\n",
        "\n",
        "- Notice that these use `=` instead of `+=`\n",
        "    - These are not accumulated. We simply pass them backward for previous timesteps or for previous layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7jweJtQ981O",
        "outputId": "fcdac30a-4dec-4bc5-ebe8-aa713e54c7c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.58475826, -0.67033156, -0.75590486],\n",
              "        [-0.12264157, -0.20364703, -0.28465248]]),\n",
              " array([[-0.32328674, -0.40886004, -0.49443334, -0.58000664],\n",
              "        [-0.57872849, -0.51852803, -0.45832757, -0.39812711]]),\n",
              " array([[ 1.91117462, -5.88346356, -4.84800998],\n",
              "        [-0.06215437,  0.33746039,  0.22700447],\n",
              "        [ 0.23218992,  0.35919733, -0.07934451],\n",
              "        [ 4.86780684,  8.3993488 , -1.25113283]]),\n",
              " array([[ 0.86417554, -3.98110726, -6.74732556, -2.22931599],\n",
              "        [-0.05965885,  0.14792449,  0.29539408,  0.11747207],\n",
              "        [-0.12693516, -0.34804089, -0.26143017,  0.05969569],\n",
              "        [-2.84879441, -7.18687201, -5.02914647,  1.51891304]]),\n",
              " array([ 7.06029576, -0.39713415, -0.37352224, -8.82692344]),\n",
              " array([ 7.06029576, -0.39713415, -0.37352224, -8.82692344]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def test_rnncell_backward_1(RNNCell):\n",
        "    layer = RNNCell(3, 4)\n",
        "    layer.weight_ih = np.array([[-0.01, -0.02, -0.03],\n",
        "                                [0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06],\n",
        "                                [0.07, 0.08, 0.09]])\n",
        "    layer.bias_ih = np.array([[-0.01, -0.02, 0.03, 0.04]])\n",
        "    layer.weight_hh = np.array([[-0.08, -0.07, -0.06, -0.05],\n",
        "                                [-0.04, -0.03, -0.02, -0.01],\n",
        "                                [0., 0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06, 0.07]])\n",
        "    layer.bias_hh = np.array([[0.01, 0.02, -0.03, -0.04]])\n",
        "    x = np.array([[1., 2., 3.],\n",
        "                  [4., 5., 6.]])\n",
        "    h_prev = np.array([[1., 2., 3., 4.],\n",
        "                    [3., 2., 1., 0.]])\n",
        "\n",
        "    grad = np.array([[ 0.402888  ,  0.01693988,  0.04669028,  0.54219921],\n",
        "                     [-0.88253697,  0.11543817, -0.70723666, -0.69393529]])\n",
        "    h_prev_l = np.array([[-0.62162275, -0.96164911,  0.21242244],\n",
        "                         [ 0.27069328, -0.83331687, -0.6866582 ]])\n",
        "    h_prev_t = np.array([[ 0.33983293,  0.93178091,  0.69990522, -0.15981829],\n",
        "                         [ 0.12239934, -0.56387259, -0.9556718 , -0.31575391]])\n",
        "\n",
        "    dx, dh = layer.backward(grad, h_prev, h_prev_l, h_prev_t)\n",
        "    return dx, dh, layer.grad_weight_ih, layer.grad_weight_hh, layer.grad_bias_ih, layer.grad_bias_hh\n",
        "\n",
        "test_rnncell_backward_1(RNNCell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF3Z8mA9981O",
        "outputId": "aae503d3-9613-471e-aae0-d047cb5b34b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[-0.58475826, -0.67033156, -0.75590486],\n",
            "       [-0.12264157, -0.20364703, -0.28465248]]), array([[-0.32328674, -0.40886004, -0.49443334, -0.58000664],\n",
            "       [-0.57872849, -0.51852803, -0.45832757, -0.39812711]]), array([[ 1.91117462, -5.88346356, -4.84800998],\n",
            "       [-0.06215437,  0.33746039,  0.22700447],\n",
            "       [ 0.23218992,  0.35919733, -0.07934451],\n",
            "       [ 4.86780684,  8.3993488 , -1.25113283]]), array([[ 0.86417554, -3.98110726, -6.74732556, -2.22931599],\n",
            "       [-0.05965885,  0.14792449,  0.29539408,  0.11747207],\n",
            "       [-0.12693516, -0.34804089, -0.26143017,  0.05969569],\n",
            "       [-2.84879441, -7.18687201, -5.02914647,  1.51891304]]), array([ 7.06029576, -0.39713415, -0.37352224, -8.82692344]), array([ 7.06029576, -0.39713415, -0.37352224, -8.82692344]))\n",
            "(array([[ -0.84617901,   7.03415296],\n",
            "       [  5.04507322,   4.78768884],\n",
            "       [ -7.43781924, -42.75449253]]), array([[-2.49541491, -3.81841158, -5.14140825,  0.59263013, -2.84836347],\n",
            "       [-0.47913155,  3.77112298,  8.02137751,  3.23359193,  5.36143606],\n",
            "       [14.70143363,  6.90866941, -0.88409481, -2.57109157, 33.59459128]]), array([[ 0.62437844, -0.55310008],\n",
            "       [-0.24461245,  2.01565815],\n",
            "       [-0.49943346, -7.65246651],\n",
            "       [ 0.27215876,  3.49236136],\n",
            "       [ 0.37842635, -0.22423791]]), array([[-0.65587378,  0.04500894, -0.40320266,  0.66034942,  0.5125324 ],\n",
            "       [-1.10598264, -2.92263378, -0.76232504,  2.43132631, -2.10462353],\n",
            "       [12.52089698,  8.2569679 ,  9.16745542,  0.11402341, -3.53133523],\n",
            "       [-5.38900605, -4.15539586, -3.91041586,  0.92575657,  0.80646862],\n",
            "       [-0.08568464, -0.47462135,  0.0164688 ,  1.4268825 , -0.59603429]]), array([ -1.01678126,   3.50027481, -12.81183184,   5.86524776,\n",
            "        -0.40241585]), array([ -1.01678126,   3.50027481, -12.81183184,   5.86524776,\n",
            "        -0.40241585]))\n",
            "(array([[-0.12407848, -0.90903931, -0.02217964, -0.02761476, -0.08555808],\n",
            "       [ 0.07810169,  0.5551553 , -0.01860884, -0.07932369, -0.09042231],\n",
            "       [ 0.2393757 ,  1.74379649,  0.02377904, -0.00317054,  0.08084875],\n",
            "       [-0.01256774, -0.11234118, -0.04097507, -0.11778905, -0.18022456]]), array([[ 0.03131774,  0.81459936],\n",
            "       [ 0.07836825, -0.56342091],\n",
            "       [-0.00317054, -1.60112224],\n",
            "       [ 0.1197996 ,  0.02226078]]), array([[ 0.57673264, -0.02464335, -0.79661899,  0.17766652,  0.02830977],\n",
            "       [ 1.51475545,  0.07878215, -0.85036402, -0.45624132, -0.692659  ]]), array([[-0.02467373,  0.56644349],\n",
            "       [ 1.42379667, -0.70234102]]), array([-1.50401683,  1.14775557]), array([-1.50401683,  1.14775557]))\n"
          ]
        }
      ],
      "source": [
        "#from tests import test_rnncell_backward_1, test_rnncell_backward_2, test_rnncell_backward_3\n",
        "\n",
        "answer_1 = test_rnncell_backward_1(RNNCell)\n",
        "answer_2 = test_rnncell_backward_2(RNNCell)\n",
        "answer_3 = test_rnncell_backward_3(RNNCell)\n",
        "print(answer_1)\n",
        "print(answer_2)\n",
        "print(answer_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "030a4a408442a23f3e5a776b9299952c",
          "grade": true,
          "grade_id": "cell-c311673f39e113c0",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "GP1ss9g4981O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f66b6f24be9057ca7a7e2a5fc9502722",
          "grade": false,
          "grade_id": "cell-11e5a02282eadbbb",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "aSDOFC6C981P"
      },
      "source": [
        "If you passed the tests above, assign the string \"Question 2 passed\" to ans2 in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "75841a4a93802d7c16701584a98047f5",
          "grade": false,
          "grade_id": "cell-21cf191c7155b854",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "KZP0F8Hh981P"
      },
      "outputs": [],
      "source": [
        "### GRADED\n",
        "\n",
        "ans2 = \"Question 2 passed\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b9ffe4cdc475069cd6345074dfcd3e44",
          "grade": true,
          "grade_id": "cell-8c2c9576705bfb64",
          "locked": true,
          "points": 25,
          "schema_version": 3,
          "solution": false
        },
        "id": "sP4UdPeg981P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3bdb7165eeccbfde9c427ac4371153a5",
          "grade": false,
          "grade_id": "cell-e92e7ce63d036867",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "Mv-FFDxY981P"
      },
      "source": [
        "# Section 2 - `RNN` layer\n",
        "\n",
        "The `RNN` object contains one or more `RNNCell`s and handles forward and backward prop over these cells.\n",
        "\n",
        "- The number of cells is specified by the `num_layers` parameter.\n",
        "- Documentation for the actual object is [here](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
        "    - Assume `bidirectional=False` for this assignment.\n",
        "        - In practice, bidirectionality is usually crucial for RNN-based layers. You'll even use a bidirectional RNN in the second part of this assignment.\n",
        "        - We choose not to have you implement it because the idea of it is simple once you understand it, but the implementation is confusing/time-consuming.\n",
        "        - For more understanding of bidirectionality, refer to lecture (or Google)!\n",
        "    - Assume `batch_first=True` for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "902872634110c72de381065b96061a0a",
          "grade": false,
          "grade_id": "cell-996d2531193e4666",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "FC_qVOiA981P"
      },
      "source": [
        "## Question 2.1 - `RNN.forward()`\n",
        "\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"images/rnn_forward.png\" width=\"350\"/>\n",
        "</div>\n",
        "\n",
        "Above is a visualization of `RNN.forward()`. We're given an input with `seq_len=3`, and we have `num_layers=2` `RNNCell`s. The first layer (first `RNNCell`) is on the bottom, and is used **3 times**, once for each step of the input.\n",
        "\n",
        "Really, the main idea is to pass each timestep of the input through each cell of your RNN in order. That's it.\n",
        "\n",
        "The hard part is indexing and carefully overwriting objects to make your code more efficient.\n",
        "\n",
        "**Pseudocode**\n",
        "- Declare a `hidden` array to store all hidden states in, and store the initial hidden state `h_0` in it (if given)\n",
        "- for each timestep `t`:\n",
        "    - declare `x_t` by getting a slice of `x` at time `t` along the appropriate axis\n",
        "        - `x_t` should be shaped `(batch_size, input_size)`\n",
        "    - for each layer `l`:\n",
        "        - set `x_t` to the output of the `l`th layer's forward pass\n",
        "            - provide as inputs `x_t` and a slice of the `hiddens` vector at time `t` and layer index `l`\n",
        "                - the `hiddens` vector slice should be shaped `(batch_size, hidden_size)`\n",
        "        - store `x_t` in the `hiddens` vector at the `t+1`th time and the `l`th layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRXquNiQ981P",
        "outputId": "37f1ab5e-3b2a-40a6-a714-92c0036f9973"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[ 0.58465959, -0.68351586,  0.59540493, -0.02945261],\n",
              "         [ 0.03991988,  0.45049385,  0.94544681,  0.82507986],\n",
              "         [ 0.9716002 ,  0.6782135 ,  0.87452011,  0.66842751],\n",
              "         [ 0.91622999,  0.83858554,  0.91754133,  0.81214755],\n",
              "         [ 0.95477401,  0.86818172,  0.90529103,  0.80123028]],\n",
              " \n",
              "        [[ 0.67953154, -0.77425086, -0.64209047,  0.4879158 ],\n",
              "         [-0.4051153 , -0.18908843,  0.77091333,  0.43887862],\n",
              "         [ 0.89971484,  0.18574096,  0.86605359,  0.59347805],\n",
              "         [ 0.78076771,  0.76497082,  0.96071053,  0.80285804],\n",
              "         [ 0.95580484,  0.84303051,  0.9127279 ,  0.78835446]]]),\n",
              " array([[[-0.04496337,  0.03866571,  0.08922074,  0.13932018],\n",
              "         [-0.02003295,  0.02673   ,  0.06193733,  0.09699118]],\n",
              " \n",
              "        [[ 0.95477401,  0.86818172,  0.90529103,  0.80123028],\n",
              "         [ 0.95580484,  0.84303051,  0.9127279 ,  0.78835446]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "def test_rnn_forward_1(RNN):\n",
        "    rnn = RNN(3, 4, num_layers=2)\n",
        "\n",
        "    rnn.layers[0].weight_ih = np.array([[-0.01, -0.02, -0.03],\n",
        "                                [0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06],\n",
        "                                [0.07, 0.08, 0.09]])\n",
        "    rnn.layers[0].bias_ih = np.array([[-0.01, -0.02, 0.03, 0.04]])\n",
        "\n",
        "    rnn.layers[0].weight_hh = np.array([[-0.08, -0.07, -0.06, -0.05],\n",
        "                                [-0.04, -0.03, -0.02, -0.01],\n",
        "                                [0., 0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06, 0.07]])\n",
        "    rnn.layers[0].bias_hh = np.array([[0.01, 0.02, -0.03, -0.04]])\n",
        "\n",
        "    rnn.layers[1].weight_ih = np.array([[-0.48600084,  0.56108125,  0.60350991, -0.12398511],\n",
        "       [-0.61426125,  0.74533839,  0.28829775,  0.39298129],\n",
        "       [-0.23660597,  0.60180463, -0.17925026, -0.91010067],\n",
        "       [ 0.32842213,  0.88810569,  0.47370219, -0.02676785]])\n",
        "    rnn.layers[1].bias_ih = np.array([[ 0.80336632,  0.5080941 , -0.02041526, -0.23457551]])\n",
        "    rnn.layers[1].weight_hh = np.array([[-0.66123341,  0.89060331,  0.52273452,  0.51176128],\n",
        "       [ 0.56754451, -0.00703578,  0.7770004 ,  0.27559471],\n",
        "       [ 0.55518779, -0.73064323,  0.9099351 ,  0.57656225],\n",
        "       [ 0.36056783, -0.05294811,  0.47201524, -0.4093564 ]])\n",
        "    rnn.layers[1].bias_hh = np.array([[-0.03656752, -0.7702112 ,  0.43095679,  0.89191036]])\n",
        "\n",
        "    x = np.array([[[ 0.54963061, -0.05740221, -0.18798255],\n",
        "        [-0.05139327,  0.31849613, -0.71975814],\n",
        "        [ 0.25741578,  0.5026123 ,  0.64851701],\n",
        "        [ 0.15833562,  0.45382923,  0.17070994],\n",
        "        [-0.02513259,  0.7292614 ,  0.84537154]],\n",
        "\n",
        "       [[ 0.96812713, -0.77282445, -0.95171846],\n",
        "        [ 0.38731339, -0.83533687,  0.99131245],\n",
        "        [ 0.04599233, -0.95944108,  0.23168402],\n",
        "        [ 0.16541892,  0.13714912, -0.95989072],\n",
        "        [ 0.51472085,  0.37945332,  0.4281448 ]]])\n",
        "    h_0 = np.array([[[-0.71008325, -0.07277072,  0.78528585, -0.35171659],\n",
        "        [ 0.21250086,  0.90839637, -0.7255993 ,  0.53047081]],\n",
        "\n",
        "       [[-0.86511414, -0.94116341, -0.36088932,  0.69915346],\n",
        "        [-0.53193134,  0.29990252, -0.27214273, -0.72523786]]])\n",
        "\n",
        "    out, hiddens = rnn.forward(x, h_0)\n",
        "    return out, hiddens\n",
        "\n",
        "test_rnn_forward_1(RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXtP-S_m981P",
        "outputId": "a7e050ca-6665-498f-9694-bb9c5735701f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[[ 0.58465959, -0.68351586,  0.59540493, -0.02945261],\n",
            "        [ 0.03991988,  0.45049385,  0.94544681,  0.82507986],\n",
            "        [ 0.9716002 ,  0.6782135 ,  0.87452011,  0.66842751],\n",
            "        [ 0.91622999,  0.83858554,  0.91754133,  0.81214755],\n",
            "        [ 0.95477401,  0.86818172,  0.90529103,  0.80123028]],\n",
            "\n",
            "       [[ 0.67953154, -0.77425086, -0.64209047,  0.4879158 ],\n",
            "        [-0.4051153 , -0.18908843,  0.77091333,  0.43887862],\n",
            "        [ 0.89971484,  0.18574096,  0.86605359,  0.59347805],\n",
            "        [ 0.78076771,  0.76497082,  0.96071053,  0.80285804],\n",
            "        [ 0.95580484,  0.84303051,  0.9127279 ,  0.78835446]]]), array([[[-0.04496337,  0.03866571,  0.08922074,  0.13932018],\n",
            "        [-0.02003295,  0.02673   ,  0.06193733,  0.09699118]],\n",
            "\n",
            "       [[ 0.95477401,  0.86818172,  0.90529103,  0.80123028],\n",
            "        [ 0.95580484,  0.84303051,  0.9127279 ,  0.78835446]]]))\n",
            "(array([[[-0.99996409, -0.0784464 ,  0.91327301,  0.95691316,\n",
            "          0.89450007],\n",
            "        [-0.10338642,  0.89094357,  0.28113123,  0.99997499,\n",
            "          0.5201417 ],\n",
            "        [-0.9953636 , -0.99975197,  0.87428212,  1.        ,\n",
            "          0.99999996],\n",
            "        [ 0.99982056,  0.99929033, -0.89844438,  0.88579268,\n",
            "         -0.99999863],\n",
            "        [-0.99999971, -0.99998575, -0.9569316 , -0.99875439,\n",
            "          0.99999486]],\n",
            "\n",
            "       [[-1.        , -0.99999919,  0.99994018,  1.        ,\n",
            "          1.        ],\n",
            "        [ 0.99999976,  0.99511573, -0.99355722,  0.99686628,\n",
            "         -0.99998772],\n",
            "        [-0.99997634, -0.99999268, -0.97978326, -0.96452611,\n",
            "          0.99996789],\n",
            "        [ 1.        ,  0.99999998, -1.        , -1.        ,\n",
            "         -1.        ],\n",
            "        [ 0.71923807, -0.99985958, -0.99999369, -0.99999997,\n",
            "          0.99998771]],\n",
            "\n",
            "       [[ 1.        ,  1.        , -1.        , -1.        ,\n",
            "         -1.        ],\n",
            "        [-0.99984218, -0.99866485, -0.9967967 , -0.99999999,\n",
            "          0.99998771],\n",
            "        [ 1.        ,  1.        , -0.99989982, -1.        ,\n",
            "         -1.        ],\n",
            "        [-0.99493713, -0.99933131, -0.99999996, -1.        ,\n",
            "          0.99998773],\n",
            "        [ 1.        ,  0.99999998, -1.        , -1.        ,\n",
            "         -1.        ]]]), array([[[-0.99999971, -0.99998575, -0.9569316 , -0.99875439,\n",
            "          0.99999486],\n",
            "        [ 0.71923807, -0.99985958, -0.99999369, -0.99999997,\n",
            "          0.99998771],\n",
            "        [ 1.        ,  0.99999998, -1.        , -1.        ,\n",
            "         -1.        ]]]))\n",
            "(array([[[-0.92214987,  0.52725146],\n",
            "        [-0.06156498,  0.44171874],\n",
            "        [-0.1737589 , -0.27000512]],\n",
            "\n",
            "       [[-0.4018654 ,  0.19944129],\n",
            "        [-0.30193885, -0.06341093],\n",
            "        [-0.63827939,  0.03383585]]]), array([[[-0.06276212,  0.69366585],\n",
            "        [ 0.18920875,  0.94440043]],\n",
            "\n",
            "       [[ 0.25714016, -0.97447423],\n",
            "        [-0.01212091, -0.96801908]],\n",
            "\n",
            "       [[-0.1737589 , -0.27000512],\n",
            "        [-0.63827939,  0.03383585]]]))\n"
          ]
        }
      ],
      "source": [
        "#from tests import test_rnn_forward_1, test_rnn_forward_2, test_rnn_forward_3\n",
        "\n",
        "answer_1 = test_rnn_forward_1(RNN)\n",
        "answer_2 = test_rnn_forward_2(RNN)\n",
        "answer_3 = test_rnn_forward_3(RNN)\n",
        "print(answer_1)\n",
        "print(answer_2)\n",
        "print(answer_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "25d74f78052d2f2059f8f28b95d431ab",
          "grade": true,
          "grade_id": "cell-996ae9cb501b08cf",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "6VdBRtor981P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "682d29b3282a6ba80731c728c50df316",
          "grade": false,
          "grade_id": "cell-b33894c8a24843a9",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "R7fzf6R7981P"
      },
      "source": [
        "If you passed the tests above, assign the string \"Question 3 passed\" to ans3 in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b7439a025c4b73700ecd3c486bf261bc",
          "grade": false,
          "grade_id": "cell-8628795b3f4c2c7b",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "Pua4pi4J981P"
      },
      "outputs": [],
      "source": [
        "### GRADED\n",
        "\n",
        "ans3 =  \"Question 3 passed\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "33d37c626bcec100d2fa04412ad65c45",
          "grade": true,
          "grade_id": "cell-1beb0db3cf2c3f8d",
          "locked": true,
          "points": 25,
          "schema_version": 3,
          "solution": false
        },
        "id": "sOUdPqxt981P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRLji6Mf981P"
      },
      "source": [
        "## Question 2.2 - RNN.backward()\n",
        "\n",
        "This is the famous \"[backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time)\" algorithm that was independently derived by hand in the 80's and 90's by multiple researchers. Although [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) software like torch's `autograd` and modern computers make calculating arbitrary derivatives trivial at this point in time.\n",
        "\n",
        "The challenge comes from all of the information being passed around and stored. Multiple timesteps, multiple layers, and hidden states for each of these layers and timesteps. You essentially need to retrace the forward diagram in reverse order, passing every necessary tensor at the right step.\n",
        "\n",
        "The indexing and explanation for this is truly unbearable. We've tried to simplify this, but the algorithm is just inherently messy.\n",
        "\n",
        "**Pseudocode**\n",
        "- Initialize `dx` and `dh_0`, the gradients of the input and initial hidden state.\n",
        "    - Assign the last layer index of `dh_0` to be `grad` (given)\n",
        "- for each timestep `t` in reverse order from `seq_len` to `1` (inclusive on both sides, see notes):\n",
        "    - for each layer `l` in reverse order from `num_layers-1` to `1` (inclusive on both sides):\n",
        "        - get `dx_t_l` and `dh_0[l]` from the backward pass of the `l`th layer\n",
        "            - provide `dh_0[l]`, `hiddens[t][l]`, `hiddens[t][l-1]`, and `hiddens[t-1][l]` as inputs\n",
        "                - See the docstring of `RNNCell.backward()` for what these are\n",
        "            - we're calculating the gradient of loss w.r.t. the current cell\n",
        "        - add `dx_t_l` to `dh_0[l-1]` using a `+=` operation\n",
        "            - because the hidden state from the previous layer impacted the current cell, we need to add its gradient (its impact) to the gradient of the loss w.r.t. previous layer's hidden state\n",
        "    - get `dx_t` and `dh_0[0]` from the backward pass of the `0`th layer\n",
        "        - provide `dh_0[0]`, `hiddens[t][0]`, `x[:,t-1,:]`, and `hiddens[t-1][0]` as inputs\n",
        "    - set `dx[:,t-1,:]` equal to `dx_t`\n",
        "        - after making it back through all the cells, we're finally back at the input\n",
        "\n",
        "**Notes:**\n",
        "- By \"inclusive on both sides\", we mean `t`$\\in$`[seq_len, seq_len-1, ..., 1]`.\n",
        "    - For `l` it'd be `l`$\\in$`[num_layers-1, num_layers-2, ..., 1]`.\n",
        "- The reason we end the inner loop at `1` is because the `0`th layer has to be given a slice from `x`.\n",
        "    - Notice that we're giving this slice of `x` as `h_prev_l` in `RNNCell.backward()`. This is the \"previous layer\", which doesn't exist before the `0`th layer. So we just give it the original input at that timestep.\n",
        "    - If you can find a way to neatly merge this into the `for` loop, hats off to you.\n",
        "- Notice: `RNNCell.backward()` handles storing gradients for us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOwfSA6V981Q",
        "outputId": "6962de24-556b-46d2-db5d-7dca26d54147"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[ 6.20279385e-05,  1.04250814e-04,  1.46473689e-04],\n",
              "         [-2.01947807e-05,  2.86394852e-05,  7.74737512e-05],\n",
              "         [ 4.30260692e-04,  6.04680222e-04,  7.79099753e-04],\n",
              "         [-1.72465287e-04, -1.07503187e-04, -4.25410876e-05],\n",
              "         [ 8.87477391e-03,  1.13015595e-02,  1.37283451e-02]],\n",
              " \n",
              "        [[-4.99980869e-04, -3.52623822e-04, -2.05266775e-04],\n",
              "         [-9.61793332e-04, -6.87768596e-04, -4.13743861e-04],\n",
              "         [ 3.01276612e-03,  4.22621242e-03,  5.43965873e-03],\n",
              "         [ 1.24645376e-03,  2.00870228e-03,  2.77095080e-03],\n",
              "         [ 1.88604891e-02,  2.41814534e-02,  2.95024177e-02]]]),\n",
              " array([[[-2.65690159e-04, -2.02933049e-04, -1.40175939e-04,\n",
              "          -7.74188289e-05],\n",
              "         [-1.98018804e-03, -1.74489055e-03, -1.50959307e-03,\n",
              "          -1.27429558e-03]],\n",
              " \n",
              "        [[ 5.98615748e-03, -4.28515946e-03,  3.59952843e-03,\n",
              "          -1.66637869e-03],\n",
              "         [ 3.03995010e-02, -2.45643144e-02,  2.57475335e-02,\n",
              "          -4.13379042e-03]]]),\n",
              " [[array([[0.0037086 , 0.04014366, 0.08134949],\n",
              "          [0.17620498, 0.0866159 , 0.2365223 ],\n",
              "          [0.09497898, 0.08670439, 0.14924411],\n",
              "          [0.05096411, 0.11848566, 0.10474897]]),\n",
              "   array([[ 0.00124296,  0.00430274, -0.00214386,  0.00207202],\n",
              "          [ 0.00663814,  0.02103044, -0.02389877,  0.00528882],\n",
              "          [ 0.00245713, -0.00160168, -0.00583053, -0.00495666],\n",
              "          [ 0.00060412, -0.01127209,  0.00654753, -0.01002118]]),\n",
              "   array([-0.03271512,  0.51802354,  0.31500666,  0.1817812 ]),\n",
              "   array([-0.03271512,  0.51802354,  0.31500666,  0.1817812 ])],\n",
              "  [array([[ 0.00253843, -0.00093234, -0.00334927, -0.00575533],\n",
              "          [-0.00478712,  0.00438   ,  0.01122608,  0.01802806],\n",
              "          [ 0.00397092, -0.00547339, -0.01133814, -0.01716301],\n",
              "          [-0.01314708,  0.01195448,  0.02898393,  0.04590566]]),\n",
              "   array([[ 0.0680666 , -0.05168504, -0.00853989,  0.02652446],\n",
              "          [ 0.14063688,  0.16257252,  0.24057404,  0.1780275 ],\n",
              "          [-0.10820908, -0.1289466 , -0.14306753, -0.11006257],\n",
              "          [ 0.23150425,  0.29994907,  0.3523657 ,  0.29142334]]),\n",
              "   array([ 0.043557  ,  0.25402595, -0.08873744,  0.40224602]),\n",
              "   array([ 0.043557  ,  0.25402595, -0.08873744,  0.40224602])]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "def test_rnn_backward_1(RNN):\n",
        "    rnn = RNN(3, 4, num_layers=2)\n",
        "\n",
        "    rnn.layers[0].weight_ih = np.array([[-0.01, -0.02, -0.03],\n",
        "                                [0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06],\n",
        "                                [0.07, 0.08, 0.09]])\n",
        "    rnn.layers[0].bias_ih = np.array([[-0.01, -0.02, 0.03, 0.04]])\n",
        "\n",
        "    rnn.layers[0].weight_hh = np.array([[-0.08, -0.07, -0.06, -0.05],\n",
        "                                [-0.04, -0.03, -0.02, -0.01],\n",
        "                                [0., 0.01, 0.02, 0.03],\n",
        "                                [0.04, 0.05, 0.06, 0.07]])\n",
        "    rnn.layers[0].bias_hh = np.array([[0.01, 0.02, -0.03, -0.04]])\n",
        "\n",
        "    rnn.layers[1].weight_ih = np.array([[-0.48600084,  0.56108125,  0.60350991, -0.12398511],\n",
        "       [-0.61426125,  0.74533839,  0.28829775,  0.39298129],\n",
        "       [-0.23660597,  0.60180463, -0.17925026, -0.91010067],\n",
        "       [ 0.32842213,  0.88810569,  0.47370219, -0.02676785]])\n",
        "    rnn.layers[1].bias_ih = np.array([[ 0.80336632,  0.5080941 , -0.02041526, -0.23457551]])\n",
        "    rnn.layers[1].weight_hh = np.array([[-0.66123341,  0.89060331,  0.52273452,  0.51176128],\n",
        "       [ 0.56754451, -0.00703578,  0.7770004 ,  0.27559471],\n",
        "       [ 0.55518779, -0.73064323,  0.9099351 ,  0.57656225],\n",
        "       [ 0.36056783, -0.05294811,  0.47201524, -0.4093564 ]])\n",
        "    rnn.layers[1].bias_hh = np.array([[-0.03656752, -0.7702112 ,  0.43095679,  0.89191036]])\n",
        "\n",
        "    x = np.array([[[ 0.54963061, -0.05740221, -0.18798255],\n",
        "        [-0.05139327,  0.31849613, -0.71975814],\n",
        "        [ 0.25741578,  0.5026123 ,  0.64851701],\n",
        "        [ 0.15833562,  0.45382923,  0.17070994],\n",
        "        [-0.02513259,  0.7292614 ,  0.84537154]],\n",
        "\n",
        "       [[ 0.96812713, -0.77282445, -0.95171846],\n",
        "        [ 0.38731339, -0.83533687,  0.99131245],\n",
        "        [ 0.04599233, -0.95944108,  0.23168402],\n",
        "        [ 0.16541892,  0.13714912, -0.95989072],\n",
        "        [ 0.51472085,  0.37945332,  0.4281448 ]]])\n",
        "    h_0 = np.array([[[-0.71008325, -0.07277072,  0.78528585, -0.35171659],\n",
        "        [ 0.21250086,  0.90839637, -0.7255993 ,  0.53047081]],\n",
        "\n",
        "       [[-0.86511414, -0.94116341, -0.36088932,  0.69915346],\n",
        "        [-0.53193134,  0.29990252, -0.27214273, -0.72523786]]])\n",
        "\n",
        "    rnn.forward(x, h_0)\n",
        "\n",
        "    grad = np.array([[-0.1, 0.2, -0.3, 0.4],\n",
        "                     [-0.4, 0.5, -0.6, 0.7]])\n",
        "\n",
        "    dx, dh_0 = rnn.backward(grad)\n",
        "\n",
        "    # Gather all the gradients in a single list\n",
        "    layer_gradients = []\n",
        "    for n in range(rnn.num_layers):\n",
        "        layer_gradients.append([rnn.layers[n].grad_weight_ih, rnn.layers[n].grad_weight_hh, rnn.layers[n].grad_bias_ih, rnn.layers[n].grad_bias_hh])\n",
        "\n",
        "    return dx, dh_0, layer_gradients\n",
        "\n",
        "# We check the input and initial hidden state gradients, as well as gradients on every RNNCell\n",
        "test_rnn_backward_1(RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nmh__QC981Q",
        "outputId": "86e526f8-44de-4b75-a722-8d50dd7222fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[[ 6.20279385e-05,  1.04250814e-04,  1.46473689e-04],\n",
            "        [-2.01947807e-05,  2.86394852e-05,  7.74737512e-05],\n",
            "        [ 4.30260692e-04,  6.04680222e-04,  7.79099753e-04],\n",
            "        [-1.72465287e-04, -1.07503187e-04, -4.25410876e-05],\n",
            "        [ 8.87477391e-03,  1.13015595e-02,  1.37283451e-02]],\n",
            "\n",
            "       [[-4.99980869e-04, -3.52623822e-04, -2.05266775e-04],\n",
            "        [-9.61793332e-04, -6.87768596e-04, -4.13743861e-04],\n",
            "        [ 3.01276612e-03,  4.22621242e-03,  5.43965873e-03],\n",
            "        [ 1.24645376e-03,  2.00870228e-03,  2.77095080e-03],\n",
            "        [ 1.88604891e-02,  2.41814534e-02,  2.95024177e-02]]]), array([[[-2.65690159e-04, -2.02933049e-04, -1.40175939e-04,\n",
            "         -7.74188289e-05],\n",
            "        [-1.98018804e-03, -1.74489055e-03, -1.50959307e-03,\n",
            "         -1.27429558e-03]],\n",
            "\n",
            "       [[ 5.98615748e-03, -4.28515946e-03,  3.59952843e-03,\n",
            "         -1.66637869e-03],\n",
            "        [ 3.03995010e-02, -2.45643144e-02,  2.57475335e-02,\n",
            "         -4.13379042e-03]]]), [[array([[0.0037086 , 0.04014366, 0.08134949],\n",
            "       [0.17620498, 0.0866159 , 0.2365223 ],\n",
            "       [0.09497898, 0.08670439, 0.14924411],\n",
            "       [0.05096411, 0.11848566, 0.10474897]]), array([[ 0.00124296,  0.00430274, -0.00214386,  0.00207202],\n",
            "       [ 0.00663814,  0.02103044, -0.02389877,  0.00528882],\n",
            "       [ 0.00245713, -0.00160168, -0.00583053, -0.00495666],\n",
            "       [ 0.00060412, -0.01127209,  0.00654753, -0.01002118]]), array([-0.03271512,  0.51802354,  0.31500666,  0.1817812 ]), array([-0.03271512,  0.51802354,  0.31500666,  0.1817812 ])], [array([[ 0.00253843, -0.00093234, -0.00334927, -0.00575533],\n",
            "       [-0.00478712,  0.00438   ,  0.01122608,  0.01802806],\n",
            "       [ 0.00397092, -0.00547339, -0.01133814, -0.01716301],\n",
            "       [-0.01314708,  0.01195448,  0.02898393,  0.04590566]]), array([[ 0.0680666 , -0.05168504, -0.00853989,  0.02652446],\n",
            "       [ 0.14063688,  0.16257252,  0.24057404,  0.1780275 ],\n",
            "       [-0.10820908, -0.1289466 , -0.14306753, -0.11006257],\n",
            "       [ 0.23150425,  0.29994907,  0.3523657 ,  0.29142334]]), array([ 0.043557  ,  0.25402595, -0.08873744,  0.40224602]), array([ 0.043557  ,  0.25402595, -0.08873744,  0.40224602])]])\n",
            "(array([[[-1.73587235e-01, -1.84859274e-01],\n",
            "        [-2.10157727e-01, -3.10148978e-01],\n",
            "        [-1.18140314e-01, -1.53273205e-01],\n",
            "        [-1.80692135e-01, -1.00724382e-01],\n",
            "        [-1.10391519e-01, -1.58259790e-01]],\n",
            "\n",
            "       [[ 3.84159435e-14,  5.13133304e-14],\n",
            "        [ 1.55790500e-10,  1.34524640e-10],\n",
            "        [ 2.08372040e-09,  1.50938990e-09],\n",
            "        [-5.69381085e-10, -4.35540199e-09],\n",
            "        [-1.27126751e-04, -1.38603488e-01]],\n",
            "\n",
            "       [[ 0.00000000e+00,  0.00000000e+00],\n",
            "        [ 2.63760419e-14,  3.70017775e-14],\n",
            "        [ 2.06188614e-12,  2.74921203e-12],\n",
            "        [ 5.34233353e-12,  5.24693442e-09],\n",
            "        [-5.03535930e-09,  1.97984650e-08]]]), array([[[ 1.64933002e-01,  6.34162948e-02, -3.81004124e-02,\n",
            "         -1.30593971e-01, -1.07463917e-01],\n",
            "        [-3.68297464e-16,  1.25290914e-14,  2.54264803e-14,\n",
            "          3.83238691e-14,  2.21218307e-21],\n",
            "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
            "          0.00000000e+00,  0.00000000e+00]]]), [[array([[ 0.0220453 , -0.0212481 ],\n",
            "       [ 0.02825181, -0.04561209],\n",
            "       [ 0.08148023,  0.00876873],\n",
            "       [ 0.02208819, -0.00707508],\n",
            "       [-0.0001367 , -0.00068082]]), array([[ 2.33326904e-02,  2.66457137e-02, -2.38787507e-02,\n",
            "        -2.43003742e-02, -2.41697650e-02],\n",
            "       [ 6.43834646e-03,  7.66293719e-04, -1.61170487e-02,\n",
            "        -3.47046676e-02, -2.20634370e-02],\n",
            "       [ 4.66043468e-02, -5.65790584e-02, -5.37644350e-02,\n",
            "        -1.65690075e-01, -6.16570714e-02],\n",
            "       [ 2.47586179e-02,  2.52033992e-02, -2.47794363e-02,\n",
            "        -2.97924309e-02, -2.96926482e-02],\n",
            "       [-6.47799920e-04, -5.01904409e-05,  4.43280848e-04,\n",
            "         2.02181324e-04,  3.43681699e-04]]), array([ 0.03143973, -0.06789479, -0.18548756, -0.0394127 , -0.0002507 ]), array([ 0.03143973, -0.06789479, -0.18548756, -0.0394127 , -0.0002507 ])]])\n",
            "(array([[[-0.00110419, -0.00511686,  0.00548371,  0.01662243,\n",
            "          0.02440453],\n",
            "        [ 0.0072472 ,  0.05298136,  0.00107765,  0.00096617,\n",
            "          0.00403238],\n",
            "        [-0.03385266, -0.24767091, -0.00539267, -0.00557853,\n",
            "         -0.02042533]],\n",
            "\n",
            "       [[-0.00189758, -0.01480979, -0.00207348, -0.0055717 ,\n",
            "         -0.00899091],\n",
            "        [ 0.00417571,  0.03071099,  0.0009727 ,  0.00160117,\n",
            "          0.00388167],\n",
            "        [ 0.00148847,  0.01129384,  0.00100915,  0.0025376 ,\n",
            "          0.00431803]]]), array([[[-0.01682939,  0.01608713],\n",
            "        [ 0.00570156,  0.00976001]],\n",
            "\n",
            "       [[ 0.07142011,  0.12067292],\n",
            "        [-0.0691196 , -0.06216279]],\n",
            "\n",
            "       [[-0.35843254, -0.03999541],\n",
            "        [ 0.21951161, -0.05354534]]]), [[array([[-0.06385484, -0.07259395, -0.02227733, -0.02130885,  0.06786622],\n",
            "       [-0.16685484,  0.06589077,  0.10530487,  0.09419441,  0.18584582]]), array([[-0.116529  , -0.07407874],\n",
            "       [-0.10230193, -0.12816288]]), array([ 0.06843035, -0.15629733]), array([ 0.06843035, -0.15629733])], [array([[-0.19095455,  0.06199189],\n",
            "       [ 0.03150829,  0.07351131]]), array([[ 0.08099256, -0.12065592],\n",
            "       [ 0.05804146,  0.00807964]]), array([0.18125098, 0.07616567]), array([0.18125098, 0.07616567])], [array([[ 0.22949343, -0.88816943],\n",
            "       [-0.11249282, -0.2744322 ]]), array([[-0.44211596,  0.57277336],\n",
            "       [-0.72555461, -0.53486989]]), array([0.91139742, 0.32281466]), array([0.91139742, 0.32281466])]])\n"
          ]
        }
      ],
      "source": [
        "#from tests import test_rnn_backward_1, test_rnn_backward_2, test_rnn_backward_3\n",
        "\n",
        "answer_1 = test_rnn_backward_1(RNN)\n",
        "answer_2 = test_rnn_backward_2(RNN)\n",
        "answer_3 = test_rnn_backward_3(RNN)\n",
        "\n",
        "print(answer_1)\n",
        "print(answer_2)\n",
        "print(answer_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dfffe7ed5ce1a3783cb67f3ccda5b158",
          "grade": true,
          "grade_id": "cell-8368c564c3ab916c",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "UtepOcft981Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b65ee8fd2f01e555067eeb4a6ca7bdf7",
          "grade": false,
          "grade_id": "cell-13e56fecad391485",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "Ofy0RC1w981Q"
      },
      "source": [
        "If you passed the tests above, assign the string \"Question 4 passed\" to ans4 in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7fabe1915b0b3ed6e59fced4f62bb864",
          "grade": false,
          "grade_id": "cell-726601d6996f22c2",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "gouZiDIz981Q"
      },
      "outputs": [],
      "source": [
        "### GRADED\n",
        "\n",
        "ans4 = \"Question 4 passed\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0da2f8ef3cb0f20b6170a3f7f322538d",
          "grade": true,
          "grade_id": "cell-b73943b76f7d9f2e",
          "locked": true,
          "points": 25,
          "schema_version": 3,
          "solution": false
        },
        "id": "gveXqJBf981Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2e9d4756c7d65f56539f3199a6cd275a",
          "grade": false,
          "grade_id": "cell-afd3e6fab528a84b",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "WY9c0mbi981Q"
      },
      "source": [
        "# Epilogue - Part A's\n",
        "\n",
        "Good work on completing all three 'Part A' portions of each assignemnt!\n",
        "\n",
        "The goal of each 'Part A' was really to convince you that deep learning is just math at work. The perception that deep learning is a completely opaque black box is misguided at best and harmful at worst. We hope you'll agree that the math and code wasn't fancy - just matrix operations, derivatives, and simple loops.\n",
        "\n",
        "If you consolidate all the code for `mytorch` you wrote throughout the course, you have a basic functioning deep learning library. If the course had more time, you'd have been able to add modules like `Dropout`, the `Adam` optimizer, different convolutions, `LSTM`, etc. We encourage you to try implementing those yourself in this same library. You can look up descriptions of how they work, and try to recreate them. To check if your implementation is correct, compare it to the actual torch's implementation. That's essentially how we developed this library.\n",
        "\n",
        "The actual PyTorch differs in one big way though - while we manually implemented backpropagation and `backward()` methods for this course, PyTorch actually often automates this process using [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html), a package for [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
        "\n",
        "For details on how `autograd` works, read the links provided. For the second link, note that `autograd` uses \"reverse accumulation\". The reason we use this instead of forward is explained in the \"reverse accumulation\" section of that link.\n",
        "\n",
        "But we're not quite done yet - the second part of this assignment still remains. We'll see you there."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "80e249822db5758e05c7a95f2378bda83bb74a36814d9a884ba3a875cd74994c"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}