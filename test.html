<!DOCTYPE html>

<!-- ######################################### -->
<!-- Code by Juan D. Correa -  Rev. March 2025 -->
<!-- ######################################### -->


<html lang="en">

<head> 
    <link rel="icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="styles.css" />
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

     
    <div class="w3-container w3-teal">
    <title>Astro Pema Software Develophment</title>
    </div>


    <style>
        body {
            font-family: Arial, sans-serif;
            background-image: url('background.jpg');
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            background-attachment: fixed;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background-color: white;
            color: black;
            text-align: center;
            padding: 15px 0;
            font-size: 1.2em;
        }

        .menu-container {
            text-align: center;
            margin-top: 15px;
        }

        .dropdown {
            display: inline-block;
            position: relative;
        }

        .dropdown button {
            background-color: white;
            color: black;
            border: none;
            padding: 10px 15px;
            cursor: pointer;
            font-size: 1em;
        }

        .dropdown-content {
            display: none;
            position: absolute;
            background-color: white;
            min-width: 150px;
            box-shadow: 0px 4px 8px rgba(0,0,0,0.2);
            z-index: 1;
        }

        .dropdown-content a {
            color: black;
            padding: 10px;
            text-decoration: none;
            display: block;
        }

        .dropdown-content a:hover {
            background-color: #ddd;
        }

        .dropdown:hover .dropdown-content {
            display: block;
        }

        article {
            background: white;
            padding: 5px;
            border-radius: 5px;
            box-shadow: 0 0 5px rgba(0, 0, 0, 0.1);
            margin-bottom: 1px;
        }

        footer {
            text-align: center;
            font-size: 0.9em;
            color: #666;
            margin-top: 20px;
        }

       .responsive-video {
           width: 60%;
           max-width: 640px;
           height: auto;
       }


h1,
.h1 {
  font-size: var(--step-5);
}

h2,
.h2 {
  font-size: var(--step-4);
}

h3,
.h3 {
  font-size: var(--step-0);
}

h4,
.h4 {
  font-size: var(--step-2);
}

h5,
.h5 {
  font-size: var(--step-0);
}

</style>
</head>



<body>

<header>
    <div>
    <h2>Python Code Public Examples</h2>
    <h5>Juan D. Correa - Python Software Developer @ Veracruz, MX & California, USA</h5>
    <h6>astropema@gmail.com</h6>
    </div>
</header>

<br>
<br>

<div class="menu-container">
        <div class="dropdown">
            <button>About</button>
            <div class="dropdown-content">
                <a href="#">Under Construction</a>
            </div>
        </div>
</div>

<br>
<br>

<article>
<br>
<button class="w3-button w3-block w3-gray">
<a href="LunarLanderV2.html"><center>Lunar Lander Simulation</center></a>
</button>
<br>
<div class="w3-panel w3-border w3-border-gray">
<h3>
"Lunar Lander is a genre of video games loosely based on the 1969 landing of the Apollo Lunar Module on the Moon. 
In Lunar Lander games, players control a spacecraft as it falls toward the surface of the Moon or other astronomical body, 
using thrusters to slow the ship's descent and control its horizontal motion to reach a safe landing area. 
Crashing into obstacles, hitting the surface at too high a velocity, or running out of fuel all result in failure. 
In some games in the genre, the ship's orientation must be adjusted as well as its horizontal and vertical velocities." Wikipedia
</div>
<center>
<video class="responsive-video" controls>
    <source src="lunar_lander_simulation_fixed.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>
</center>
</article>

<br>


<div>
<article>
<button class="w3-button w3-block w3-gray">
<a href="CartPoleSimulation.html"><center>Cart Pole Simulation</center></a>
</button>
<br><br>
<div class="w3-panel w3-border w3-border-gray">
<h3>"An inverted pendulum is a pendulum that has its center of mass above its pivot point. It is unstable and falls over without additional help. 
It can be suspended stably in this inverted position by using a control system to monitor the angle of the pole and move the pivot point 
horizontally back under the center of mass when it starts to fall over, keeping it balanced. The inverted pendulum is a classic problem 
in dynamics and control theory and is used as a benchmark for testing control strategies. It is often implemented with the pivot point 
mounted on a cart that can move horizontally under control of an electronic servo system as shown in the photo; 
this is called a cart and pole apparatus." Wikipedia</h3>
</div>
<center>
<video class="responsive-video" controls>
     <source src="cartpole_simulation_fixed.mp4" type="video/mp4">
            Your browser does not support the video tag.
</video>
</center>
</article>
</div>

<br>

<div>
<article>
<button class="w3-button w3-block w3-gray">
<a href="JDCorreaMarch2025.pdf"><center>MIT DSML Capstone Project</center></a>
</button>
<br>
<div class="w3-panel w3-border w3-border-gray">
<h3>
This project aims to build and evaluate a recommendation system using the Amazon
product ratings dataset. We will explore multiple models, including KNN and SVD, and
evaluate their performance using metrics such as RMSE, precision, recall, and F1 score.
Hyperparameter tuning and cross-validation will be performed to optimize the models.

Objectives
Explore and preprocess the Amazon product ratings dataset.
Implement and evaluate different recommendation models (KNN, SVD).
Perform hyperparameter tuning and cross-validation to optimize the models.
Compare the performance of different models and provide recommendations
</h3>
</div>
</article>
</div>

<br>

<div>
<article>
<button class="w3-button w3-block w3-gray">
<a href="MLS_Yelp_Reviews_Notebook.pdf"><center>Yelp Restaurant Recommendation System</center></a>
</button>
<br>
<div class="w3-panel w3-border w3-border-gray">
<h3>
Yelp was founded in 2004 to help people find great local businesses. Today, the website and their mobile application publish crowd-sourced 
reviews about local businesses as well as certain metadata about them that can help in customer's decision-making process. 
Yelp uses automated software to recommend the most helpful and reliable reviews for the Yelp community from such a large and diverse dataset.

The Yelp dataset is a large collection of user reviews, business metadata, business check-ins, users' social network data, 
user tips for businesses across 10 cities spread across 4 countries. The original dataset is very huge with ~ 11GB of data. 
In this case study, we will only use a subset of data due to the hardware limitations.
</h3>
</div>
</article>
</div>

<br>

<div>
<article>
<button class="w3-button w3-block w3-gray">
<a href="KALMANFILTER3DLocationTracking.pdf"><center>3D Location tracking</center></a>
</button>
<br>
<div class="w3-panel w3-border w3-border-gray">
<h3>
Our goal is to track the location (and velocity) of a moving object, e.g. a ball, in a 3-dimensional space. We will allow gravity 
to act on the ball and the initial position and velocities are assumed to be known. We will be using noisy location estimates using 
a (simulated) sensor. The objective is to estimate the true location (and velocity) of the ball in 3D space.
</h3>
</div>
</article>
</div>

<br>

<div>
<article>
<button class="w3-button w3-block w3-gray">
<a href="KALMANFILTER_constantvelocity.pdf"><center>2D Location tracking</center></a>
</button>
<br>
<div class="w3-panel w3-border w3-border-gray">
<h3>
Our goal is to track the location (and velocity) of a moving object, e.g. a car, in a 2-dimensional space. 
The only information available to us is the initial location (and velocity) and a series of noisy measurements 
of the velocity as the object moves in space. The key assumption of this problem is that the true velocity of the object is 
known to be a constant. However, the constant velocity is not known to us.
</h3>
</div>
</article>
</div>

<br>

<div>
<article>
<button class="w3-button w3-block w3-gray">
<a href="Unemployment_Rate_Prediction.pdf"><center>Unemployement Rate Prediction Study</center></a>
</button>
<br>
<div class="w3-panel w3-border w3-border-gray">
<h3>
The Hidden Markov Model (HMM) is a probabilistic model that is used to explain or derive the probability of any random process. 
It basically states that an observed event will be related to a set of probability distributions rather than its step-by-step status. 
Assume the system being modelled is a Markov chain, with some hidden states in the process. In that case, hidden states are a process 
that is dependent on the main Markov process/chain.

The primary goal of the HMM is to discover information about a Markov chain by observing its hidden states. Considering a Markov 
process X with hidden states Y, the HMM establishes that the probability distribution of Y for each time stamp must not be dependent 
on the history of X at that time.
</h3>
</div>
</article>
</div>

<br>

<div>
<article>
     <a href="Case_Study_Employee_Attrition.html"><center><h4>Employee Attrition Prediction Case</h4></center></a>
<br>
<p>
<h3> McCurr Health Consultancy is an MNC that has thousands of employees spread across the globe. The company believes in hiring the best talent 
available and retaining them for as long as possible. A huge amount of resources is spent on retaining existing employees through various initiatives. 
The Head of People Operations wants to bring down the cost of retaining employees. For this, he proposes limiting the incentives to only 
those employees who are at risk of attrition. As a recently hired Data Scientist in the People Operations Department, 
you have been asked to identify patterns in characteristics of employees who leave the organization. 
Also, you have to use this information to predict if an employee is at risk of attrition. 
This information will be used to target them with incentives.</h3>
</p>
</article>
</div>

<br>

<div>
<article>
  <a href="Credit_Card_Users_Churn_Prediction.pdf"><center><h4>Credit Card Users Churn Prediction</h4></center></a>
<br>
<p>
<h3>The Thera bank recently saw a steep decline in the number of credit card users. 
Credit cards are a good source of money for banks because of several types of fees charged by the banks, such as annual fees,
 balance transfer costs, cash advance fees, late payment fees, international transaction fees, and others. 
Some costs are assessed to all users regardless of consumption, while others are assessed only under specific conditions.

Customers leaving credit card services would result in a loss for the bank, so the bank wants to analyse customer data to identify customers 
who will leave their credit card services and the reasons for doing so – so that the bank can improve in those areas.

You as a Data scientist at Thera bank need to come up with a classification model that will help the bank improve its services so that 
customers do not renounce their credit cards. You need to identify the best possible model that will give the required performance.</h3>
</p>
</article>
</div>

<br>

<div>
<article>
<a href="Audio_MNIST_Digit_Recognition.pdf"><center><h4>Audio MNIST Digit Recognition</center></h4></a>
<br>
<p>
<h3>
In the past decades, significant advances have been achieved in the area of audio recognition and a lot of research is going on 
globally to recognize audio data or speech using Deep Learning. The most common use case in this field is converting audio to 
spectrograms and vice versa.

Audio in its raw form is usually a wave and to capture that using a data structure, we need to have a huge array of amplitudes even 
for a very short audio clip. Although it depends on the sampling rate of the sound wave, this structured data conversion for any 
audio wave is very voluminous even for low sampling rates. So it becomes a problem to store and computationally very expensive to do even 
simple calculations on such data.

One of the best economical alternatives to this is using spectrograms. Spectrograms are created by doing Fourier or Short Time Fourier Transforms
 on sound waves. There are various kinds of spectrograms but the ones we will be using are called MFCC spectrograms. 

To put it in simple terms, a spectrogram is a way to visually encapsulate audio data. It is a graph on a 2-D plane where 
the X-axis represents time and the Y-axis represents Mel Coefficients. But since it is continuous on a 2-D plane, we can treat this as an image.
</p>
</h3>
</article>
</div>

<br>

<div>
<article>
<a href="FoodImageClassificationV1.pdf"><center><h4>Food Image Classification Project</center></h4></a>
<br>
<p>
<h3>
Image classification has become less complicated with deep learning and the availability of larger datasets and computational assets. 
The Convolution neural network is the most popular and extensively used image classification technique in the latest day.

Clicks is a stock photography company and is an online source of images available for people and companies to download. 
Photographers from all over the world upload food-related images to the stock photography agency every day. Since the volume of the 
images that get uploaded daily will be high, it will be difficult for anyone to label the images manually.
</h3>
</p>
</article>
</div>

<br>

<div>
<article>
<a href="BrainTumorImageClass.pdf"><center><h4>Brain Tumor Image Classifier Study</center></h4></a>
<br>
<p>
<h3>
In this notebook, we will build an image classifier that can distinguish Pituitary Tumor from "No Tumor" MRI Scan images.

The dataset used in this notebook is available for download from Kaggle.

Although this dataset actually has a total of 3,264 images belonging to 4 classes - Glioma Tumor, 
Meningioma Tumor, Pituitary Tumor and No Tumor, for this project we have only taken two classes, and we are 
building a binary classification model to classify between the Pituitary Tumor category vs No Tumor.

For this project, we will only use 1000 of these images (830 training images and 170 Testing images). For the training dataset, 
we will take 395 MRI scans of No Tumor and 435 MRI scans of Pituitary Tumor. In our problem, we will also be using Data Augmentation 
to prevent overfitting, and to make our model model more generalised and robust.

We will use this to build an image classification model for this problem statement, and then show how we can improve our performance 
by simply "importing" a popular pre-trained model architecture and leveraging the idea of Transfer Learning.
</h3>
</p>
</article>
</div>


<br>

<div>
<article>
<a href="PlantSeedlingsClassificationOrigAndCorrectedCode.pdf"><center><h4>Plant Seedlings Clasification Project</center></h4></a>
<br>
<p>
<h3>
In recent times, the field of agriculture has been in urgent need of modernizing, since the amount of manual work people need to put in 
to check if plants are growing correctly is still highly extensive. Despite several advances in agricultural technology, 
people working in the agricultural industry still need to have the ability to sort and recognize different plants and weeds, 
which takes a lot of time and effort in the long term. The potential is ripe for this trillion-dollar industry to be greatly 
impacted by technological innovations that cut down on the requirement for manual labor, and this is where Artificial Intelligence 
can actually benefit the workers in this field, as the time and energy required to identify plant seedlings will be greatly shortened 
by the use of AI and Deep Learning. The ability to do so far more efficiently and even more effectively than experienced manual labor, 
could lead to better crop yields, the freeing up of human inolvement for higher-order agricultural decision making, and in the long 
term will result in more sustainable environmental practices in agriculture as well.
</h3>
</p>
</article>
<div>


<br>

<div>
<article>
 <a href="CNN_Practice_Project_SVHN.pdf"><center><h4>Convolutional Neural Networks: Street View Housing Number Digit Recognition</center></h4></a>
<br>
<p>
<h3>
One of the most interesting tasks in deep learning is to recognize objects in natural scenes. The ability to process visual 
information using machine learning algorithms can be very useful as demonstrated in various applications.

The SVHN dataset contains over 600,000 labeled digits cropped from street-level photos. It is one of the most popular image recognition datasets. 
It has been used in neural networks created by Google to improve the map quality by automatically transcribing the address numbers from a patch of pixels.
 The transcribed number with a known street address helps pinpoint the location of the building it represents.
</h3>
</p>
</article>
</div>

<br>

<div>
<article>
  <a href="ProductSegmentation.pdf"><center><h4>Product Segementation Study</center></h4></a>
<br>
<p>
<h3>
When you think of sneakers for a trip, the importance of good footwear cannot be discarded, and the obvious brands that come to mind are 
Adidas and Nike. Adidas vs Nike is a constant debate as the two giants in the apparel market, with a large market cap and market share, 
battle it out to come on top. As a newly hired Data Scientist in a market research company, you have been given the task of extracting 
insights from the data of men's and women's shoes, and grouping products together to identify similarities and differences between the 
product range of these renowned brands.
</h3>
</p>
</article>
</div>

<br>

<div>
<article>
<a href="Network_Stock_Portfolio_Optimization.pdf"><center><h4>Network Stock Portafolio Optimization</center></h4></a>
<br>
<p>
<h3>
Active investing in the asset management industry aims to beat the stock market’s average returns, for which portfolio managers 
track a particular index and try to beat that index by creating their own portfolios.

Portfolio construction involves selection of stocks that have a higher probability of giving better returns in comparison to the tracking 
index, like S&P 500. In this project, we will use the concept of Network Analysis to select a basket of stocks and create two portfolios. 
We will then simulate portfolio value by investing a certain amount, keeping the portfolio for an entire year and we will then compare it 
against the S&P 500 index.
</h3>
</p>
</article>
<div>

<br>

<div>
<article>
 <a href="Notebook_Inferential_Statistics.pdf"><center><h4>Inferential Statistics Study</center></h4></a>
<br>
<p>
<h3>
Problem statement
80% of all the visitors to Lavista Museum end up buying souvenirs from the souvenir shop at the Museum. On the coming Sunday, if a random sample of 10 visitors is picked:

1. Find the probability that every visitor will end up buying from the souvenir shop.  
2. Find the probability that a maximum of 7 visitors will buy souvenirs from the souvenir shop.
</h3>
</p>
</article>
</div>

<br>

<div>
<article>
<center><h4>
<a href=CAVIAR_CASE_STUDY.html>Graphical study of a time-varying criminal network.
</center></h4>
</a>
<br> 
<p>
<h3>
In this problem, we will study a time-varying criminal network that is repeatedly disturbed by police forces. The data for this problem can be found in CAVIAR.zip.

Background

Here is some information on the CAVIAR project and the role of certain individuals arrested following the investigation. 
This investigation lasted two years and ran from 1994 to 1996. The operation brought together investigation units of the Montr´eal police and the Royal Canadian 
Mounted Police of Canada
</h3>
</p>
</article>
</div>

<br>

<div>
<article>
<a href=HouseCostPredictor.html><center><h4>House Costs Predictor Project</center></h4></a>
<br>
<h3>In this exercise i will build a neural network that predicts the price of a house according to a simple formula.</h3>
</article>
</div>
</p>


<div>
<article>
<a href=MyDeepHelloWorld.html><center><h4>Hello World Deep Learning Project</center></h4></a>
<br>
<p>
<h3>The challenge in this exercise was to design and understand neural networks for both simple and complex relationships 
between input and output data.

A TensorFlow-based linear model that learned a quadratic function.

A PyTorch-based feedforward neural network capable of more complex non-linear approximations.

Both examples demonstrated the core principles of neural networks, including:

Forward propagation: Passing input through layers to compute outputs.

Activation functions: Introducing non-linearity to learn more complex patterns.

Weight and bias analysis: Understanding how the network learns and encodes functions internally.</h3>
</p>
</article>
</div>

<br>

<div>
<article>
<a href=Meteorites.html><center><h4>Meteorites Data Set Project</center></h4></a>
<br>

<h3>

Geographic Spread:
<br>
The spread is fairly dense across multiple continents but lacks representation near polar regions. Meteorites in this cluster seem geographically diverse but 
centered around lower latitudes (between 0° and 66°). 
<br><br>
Summary Statistics:
<br>
Mean Log(Mass): 3.40 (equivalent to roughly 30 grams). The cluster has a maximum log mass of 5.77, indicating smaller meteorites compared to Cluster 2. 
<br>
Latitudes: Range from -35° to 66°, indicating concentration around temperate and equatorial regions. Longitudes: More evenly distributed, but clustered 
around regions with longitudes between -137° and 174°. Comparison to Cluster 2:
<br>
Cluster 1 contains smaller meteorites on average. The geographic distribution is different, with fewer meteorites in the southern hemisphere compared to Cluster 2.
<br><br>
Overview of Analysis Steps: Data Preparation:
<br>
Cleaned the meteorite dataset. Transformed mass values using a log scale to reduce skewness. Regression Analysis:
<br>
Explored linear, random forest, and gradient boosting models to predict meteorite mass. Found that geographic (latitude/longitude) and temporal (year) 
features influenced predictions the most. Feature Importance Analysis:
<br>
Latitude and year emerged as top contributors to predicting meteorite mass. Recclass categories and fall types had smaller but noticeable impacts. 
<br><br>
Clustering Analysis:
<br>
Applied both K-Means and DBSCAN clustering. K-Means revealed four clusters with varying average mass and geographic patterns. DBSCAN clustered most points 
into a single high-density group. Cluster-Specific Insights:
<br>
Cluster 2 contained the largest meteorites, primarily of iron-based classifications. Cluster 1 had smaller meteorites on average and was geographically 
centered around temperate regions. Visualizations:
<br>
We used scatter plots, heatmaps, and boxplots to explore geographic, class, and mass distributions across clusters. Reflection and Key Takeaways: Regional Biases:
<br>
Meteorite discoveries are concentrated on landmasses, likely due to easier accessibility and research focus. Polar and oceanic regions are underrepresented in the data. 
<br><br>
Cluster Characteristics:
<br>
Larger, iron-based meteorites often form distinct clusters. Smaller meteorites show greater geographic and class diversity. Model and Feature Limitations:
<br>
Our models performed moderately well, with R² values around 0.6. Additional features, such as environmental factors or discovery methods, could improve predictions. 
<br><br>
Future Directions:
<br>
Tuning clustering parameters further. Exploring meteorite discovery trends over time. Expanding the dataset with external features (e.g., terrain or meteor shower data).
</h3>
</article>
</div>


<br>

<div>
<article>
  <center><h4>
  <a href="Hospital_LOS_Prediction_Part1.pdf">Study 1</a>   //
  <a href="Hospital_LOS_Prediction_Part2.pdf">Study 2</a>   //
  <a href="Hospital_LOS_Prediction_Part3.pdf">Study 3</a>
  </h4></center>
<br>
<p>
<h3>
Hospital management is a vital area that gained a lot of attention during the COVID-19 pandemic. Inefficient distribution of resources like beds, 
ventilators might lead to a lot of complications. However, this can be mitigated by predicting the length of stay (LOS) of a patient before 
getting admitted. Once this is determined, the hospital can plan a suitable treatment, resources, and staff to reduce the LOS and increase the chances 
of recovery. The rooms and bed can also be planned in accordance with that.

HealthPlus hospital has been incurring a lot of losses in revenue and life due to its inefficient management system. 

They have been unsuccessful in allocating pieces of equipment, beds, and hospital staff fairly. A system that could estimate the length of stay (LOS) 
of a patient can solve this problem to a great extent.
</h3>
</p>
</article>
</div>

<br>



<div>
<article>
<a href="SuperKartSales.html"><center><h4>Super Kart Sales Data Set Study</center></h4></a>
<br>
<p>
<h3>
A sales forecast is a prediction of future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. 
Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. It is extremely important for a company to make an 
accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their future course of action. Forecasting 
helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials. 
An accurate sales forecast process has many benefits which include improved decision-making about the future and reduction of sales pipeline and forecast risks. 
Moreover, it helps to reduce the time spent in planning territory coverage and establish benchmarks that can be used to assess trends in the future.
</h3>
</p>
</article>
<div>

<br>


<div>
<article>
<a href="V2Cars4U_Hands_on_Project.pdf"><center><h4>Cars 4U Project</center></h4></a>
<br>
<p>
<h3>
Context

There is a huge demand for used cars in the Indian Market today. As sales of new cars have slowed down in the recent past, the pre-owned car market has continued 
to grow over the past years and is larger than the new car market now. Cars4U is a budding tech start-up that aims to find footholes in this market.

In 2018-19, while new car sales were recorded at 3.6 million units, around 4 million second-hand cars were bought and sold. There is a slowdown in new car sales 
and that could mean that the demand is shifting towards the pre-owned market. In fact, some car owners replace their old cars with pre-owned cars instead of buying 
new ones. Unlike new cars, where price and supply are fairly deterministic and managed by OEMs (Original Equipment Manufacturer) except for dealership level discounts 
which come into play only in the last stage of the customer journey. Used cars are very different beasts with huge uncertainty in both pricing and supply. Keeping this 
in mind, the pricing scheme of these used cars becomes important in order to grow in the market.
</h3>
</p>
</article>
</div>

<br>

<div>
<article>
<a href="ExtraaLean.html"><center><h4>Extra Lean Data Set Study</center></h4></a>
<br>
<p>
<h3>
ExtraaLearn is an initial stage startup that offers programs on cutting-edge technologies to students and professionals to help them upskill/reskill. With a large 
number of leads being generated on a regular basis, one of the issues faced by ExtraaLearn is to identify which of the leads are more likely to convert so that 
they can allocate resources accordingly. You, as a data scientist at ExtraaLearn, have been provided the leads data to:

Analyze and build an ML model to help identify which leads are more likely to convert to paid customers,
Find the factors driving the lead conversion process
Create a profile of the leads which are likely to convert
</h3>
</p>
</article>
</div>

<br>

<div>
<article>
 <a href="RegressionAuto.pdf"><center><h4>Linear Regression Study</center></h4></a>
<br> 
<h3> In this case study, we aim to construct a linear model that explains the relationship a car's mileage (mpg) has with its other attributes</h3>
</article>
</div>

<br>

<div>
<article>
<a href="HumanGeneratedText.pdf"><center><h4>Human Generated Text Clustering</center></h4></a>
<br>
<h3>
Latent Dirichlet allocation (LDA) is a generative statistical model in natural language processing, and can be used to discover ‘topics’ in a large set of documents. This is first presented by David Blei, Andrew Ng, and Michael Jordan.

The key idea is that if we see a ‘topic’ as a collection of certain words, we can look at each document as a collection of topics, the proportion of each topic depends on the proportion of words in the document that are associated with that topic. For example, the ‘sports’ topic may consist of the words: tennis, football, gymnastics. When given a set of documents, we can calculate the posterior distribution for the topics. In the original LDA paper, this is done using a coordinate descent algorithm for mean-field variational inference, and later on researchers also used Gibbs Sampling and expectation propagation. In this tutorial we will be looking only at Stochastic Variational Inference for LDA. SVI was first published in 2013 by Matt Hoffman, David Blei, Chong Wang, and John Paisley.

Traditional coordinate-descent variational inference requires each update to be carried out with all of the data, and these updates become inefficient when the dataset gets large as each update scales linearly with the size of the data. The key idea with SVI is to update global variational parameters more frequently. Using local and global parameters, and given the dataset with a known number of datapoints, we could randomly take 1 data point at a time, update the local parameter, and project the change into the global parameters. Like traditional coordinate-descent variational inference, this is done until the result converges, i.e., the change in the global parameters is smaller than a certain value. The implementation we will be talking about is a naive implementation of the algorithm described in the original paper
</h3>
</article>
</div>

<br>
<div>
<article>
<a href="PCAandKMeansDecipherGenome.pdf"><center><h4>PCA and K-Means Genome Project</center></h4></a>
<br>
<h3>

The work starts with a fragment of the genomic sequence of the bacterium Caulobacter Crescentus. 
This sequence is given as a long text file (300 kb), and the task is to look at the file and ensure that the text uses the alphabet 
of four letters (A, C, G and T), and that these letters are used without spaces. It is noticeable that, although the text seems to be random, 
it is well organized, but we cannot understand it without special tools. Statistical methods may help us do so.
<br>
In this case study we accept data from a genome and have the goal of identifying useful genes versus noise. Unfortunately, we don't know 
which sequences of genes are useful, so we have to use Unsupervised Learning to infer this.
<br>
In this notebook we walk through the following series of steps:
<br>
First, the data is imported and prepared. Initially the sequence, a single string, is split into non-overlapping substrings of length 300, 
and we may then count the combinations of the distinct 1, 2, 3, and 4-length sequences of base pairs which appear in each possible substring.
PCA is performed to try to identify the internal structure of the data.
<br>
Finally, if PCA reveals some internal structure then we'll apply Clustering techniques to the dataset.
</h3>
</article>
</div>

<br>

<div>
<article>
<a href="Practice_Case_Study_Clustering.pdf"><center><h4>Socio-economic Factors & Geographic Clustering</center></h4></a>
<br>
<h3>
The study of socio-economic factors is foundational to understanding and shaping the future of societies and hence of extreme interest 
to various government and non-government institutions. While GDP is one of the important measures used in one of the popular economic 
vernacular, it is not the only measure of the growth and the state of an economy. 
<br>
This case study aims to deep dive into one such dataset that contains various socio-economic attributes for countries around the world.
</h3>
</article>
</div>

<br>

<div>
<article>
 <a href="ClusterStudy.pdf"><center><h4>Clusters Studies</center></h4></a>
<br> 
<h3>

Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in>

Cluster analysis refers to a family of algorithms and tasks rather than one specific algorithm. It can be achieved by various algorithms that differ signific>

Besides the term clustering, there is a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek>

Cluster analysis originated in anthropology by Driver and Kroeber in 1932

The notion of a "cluster" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms.
</h3>
</article>
</div>

<br>


<div>
<article>
<a href="PCA.pdf"><center><h4>Introduction to Principal Componet Analysis</center></h4></a>
<br>
<h3>
In this case study, we will use the Air pollution dataset which contains information about 13 months of data on major 
pollutants and meteorological levels of a city.
</h3>
</article>
</div>

<br>

<div>
<article>
<a href="Notebook_Hypothesis_Testing.pdf"><center><h4>Hypotesis Testing Study</center></h4></a>
<br>
<h3>
Hypothesis testing is a statistical method used to determine if there's enough evidence to support a claim about a population parameter, 
by comparing the data to a null hypothesis and an alternative hypothesis, and drawing conclusions based on a calculated test statistic. 
</h3>
</article>
</div>

<br>

<div>
<article>
<a href="StatsLibs.pdf"><center><h4>Statistical Libraries Study</center></h4></a>
<br>
<h3>A study of some of the Statistics libs available for python.</h3>  
</article>
</div>

<br>
<br>


<div class="w3-container w3-teal">
<footer>
 <center><h3>March &copy; 2025</center></h3>
</footer>
</div>


</body>
</html>
